{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre All that:\n",
    "Convert Cell names to Column names in the formulas \n",
    "(Think about how we can keep all formulas), get all formulas, pass them through an llm and make it remove the redundant ones \n",
    "\n",
    "\n",
    "Take each table, pass through an LLM:\n",
    "\n",
    "Ask it if its got columns around \"SKU, location/warehouse, quantity\" - for inventory planning (boolean)\n",
    "Ask it to mark other tables if they're related to inventory planning (boolean)\n",
    "\n",
    "For all 1s:\n",
    "    Pass table and metadata through llm and identify specific data ranges by their column labels and formulas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sheet: INVENTORY SNAPSHOT\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to analyze the sheet named \"INVENTORY SNAPSHOT\" to generate Python code that will help extract tables, column names, and formulas from it. \n",
      "Action: AnalyzeSheetAndGenerateCode\n",
      "Action Input: \"INVENTORY SNAPSHOT\"\u001b[0mProcessing sheet: INVENTORY SNAPSHOT\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mcontent='To accomplish the tasks you\\'ve outlined, we will follow a structured approach. Below is a breakdown of the steps involved, along with the corresponding Python code to extract the tables, their column names, and formulas programmatically.\\n\\n### Step 1: Detect Tables in the Data\\nFrom the provided data, we can identify that there are multiple tables based on the presence of headers and structured data. The first table appears to start from the row containing \"SKU Description\" and continues until the end of the relevant data.\\n\\n### Step 2: Extract Table Column Names\\nThe column names can be extracted from the row that contains the headers. In this case, the relevant headers are:\\n- SKU Description\\n- Ref Code\\n- Item Code\\n- Comarco\\n- Lineage\\n- TTL in storage\\n- Dot\\n- Total On Hand Inventory\\n- Available\\n- Open PO to Dot\\n- Open Comarco PO\\n- Upcoming Dot Total\\n- Upcoming Dot DOH\\n\\n### Step 3: Identify Unique Formulas and Dependencies\\nWe will analyze the rows below the header to identify unique formulas and their dependencies. Formulas often reference other cells, and we will extract these references.\\n\\n### Step 4: Generate Python Code\\nThe following Python code will help extract the tables, their column names, and formulas programmatically.\\n\\n```python\\nimport pandas as pd\\nimport re\\n\\n# Sample data as a list of lists (you would typically read this from a file)\\ndata = [\\n    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, None, \\'2025-01-10 00:00:00\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, \\'45 = antsy\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, \\'SKU Description\\', \\'Ref Code\\', \\'Item Code\\', \\'Item Code\\', \\'Comarco\\', \\'Lineage\\', \\'TTL in storage\\', \\'Dot\\', \\'Total On Hand Inventory\\', \\'available\\', None, \\'Open PO to Dot\\', \\'Open Comarco PO\\', None, None, None, \\'Upcoming Dot Total\\', \\'Upcoming Dot DOH\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    # ... (rest of the data)\\n]\\n\\n# Convert the data into a DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Step 1: Detect the start of the table\\nheader_row_index = df[df[0] == \\'SKU Description\\'].index[0]\\ntable_df = df.iloc[header_row_index:]\\n\\n# Step 2: Extract column names\\ncolumn_names = table_df.iloc[0].tolist()\\ntable_df.columns = column_names\\ntable_df = table_df[1:]  # Remove the header row from the data\\n\\n# Step 3: Identify unique formulas and dependencies\\nformulas = {}\\nfor col in table_df.columns:\\n    if table_df[col].dtype == \\'object\\':\\n        for item in table_df[col]:\\n            if isinstance(item, str) and \\'=\\' in item:\\n                # Extract unique formulas\\n                formulas[item] = re.findall(r\\'([A-Z]+[0-9]+)\\', item)\\n\\n# Step 4: Print results\\nprint(\"Column Names:\")\\nprint(column_names)\\nprint(\"\\\\nUnique Formulas and Dependencies:\")\\nfor formula, dependencies in formulas.items():\\n    print(f\"Formula: {formula}, Dependencies: {dependencies}\")\\n\\n# Optionally, you can save the extracted data to a new CSV or Excel file\\n# table_df.to_csv(\\'extracted_inventory_snapshot.csv\\', index=False)\\n```\\n\\n### Explanation of the Code:\\n1. **Data Preparation**: The data is represented as a list of lists, which can be read from a file in practice.\\n2. **DataFrame Creation**: The data is converted into a Pandas DataFrame for easier manipulation.\\n3. **Table Detection**: The code identifies the row containing the headers and extracts the relevant data.\\n4. **Column Name Extraction**: The column names are extracted and set as the DataFrame\\'s columns.\\n5. **Formula Identification**: The code iterates through each column to find unique formulas and their dependencies using regular expressions.\\n6. **Output**: The column names and unique formulas with dependencies are printed.\\n\\nThis code provides a structured way to extract and analyze the inventory snapshot data programmatically. You can modify the data input section to read from an actual file as needed.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1022, 'prompt_tokens': 5778, 'total_tokens': 6800, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-54fb3159-332e-4793-a6a4-2cab2478491e-0'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
      "\n",
      "Final Answer: The analysis of the sheet \"INVENTORY SNAPSHOT\" has generated the following Python code to extract tables, column names, and formulas:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import re\n",
      "\n",
      "# Sample data as a list of lists (you would typically read this from a file)\n",
      "data = [\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, '2025-01-10 00:00:00', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '45 = antsy', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, 'SKU Description', 'Ref Code', 'Item Code', 'Item Code', 'Comarco', 'Lineage', 'TTL in storage', 'Dot', 'Total On Hand Inventory', 'available', None, 'Open PO to Dot', 'Open Comarco PO', None, None, None, 'Upcoming Dot Total', 'Upcoming Dot DOH', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    # ... (rest of the data)\n",
      "]\n",
      "\n",
      "# Convert the data into a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Step 1: Detect the start of the table\n",
      "header_row_index = df[df[0] == 'SKU Description'].index[0]\n",
      "table_df = df.iloc[header_row_index:]\n",
      "\n",
      "# Step 2: Extract column names\n",
      "column_names = table_df.iloc[0].tolist()\n",
      "table_df.columns = column_names\n",
      "table_df = table_df[1:]  # Remove the header row from the data\n",
      "\n",
      "# Step 3: Identify unique formulas and dependencies\n",
      "formulas = {}\n",
      "for col in table_df.columns:\n",
      "    if table_df[col].dtype == 'object':\n",
      "        for item in table_df[col]:\n",
      "            if isinstance(item, str) and '=' in item:\n",
      "                # Extract unique formulas\n",
      "                formulas[item] = re.findall(r'([A-Z]+[0-9]+)', item)\n",
      "\n",
      "# Step 4: Print results\n",
      "print(\"Column Names:\")\n",
      "print(column_names)\n",
      "print(\"\\nUnique Formulas and Dependencies:\")\n",
      "for formula, dependencies in formulas.items():\n",
      "    print(f\"Formula: {formula}, Dependencies: {dependencies}\")\n",
      "\n",
      "# Optionally, you can save the extracted data to a new CSV or Excel file\n",
      "# table_df.to_csv('extracted_inventory_snapshot.csv', index=False)\n",
      "```\n",
      "\n",
      "This code provides a structured way to extract and analyze the inventory snapshot data programmatically. You can modify the data input section to read from an actual file as needed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The analysis of the sheet \"INVENTORY SNAPSHOT\" has generated the following Python code to extract tables, column names, and formulas:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import re\n",
      "\n",
      "# Sample data as a list of lists (you would typically read this from a file)\n",
      "data = [\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, '2025-01-10 00:00:00', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '45 = antsy', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, 'SKU Description', 'Ref Code', 'Item Code', 'Item Code', 'Comarco', 'Lineage', 'TTL in storage', 'Dot', 'Total On Hand Inventory', 'available', None, 'Open PO to Dot', 'Open Comarco PO', None, None, None, 'Upcoming Dot Total', 'Upcoming Dot DOH', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    # ... (rest of the data)\n",
      "]\n",
      "\n",
      "# Convert the data into a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Step 1: Detect the start of the table\n",
      "header_row_index = df[df[0] == 'SKU Description'].index[0]\n",
      "table_df = df.iloc[header_row_index:]\n",
      "\n",
      "# Step 2: Extract column names\n",
      "column_names = table_df.iloc[0].tolist()\n",
      "table_df.columns = column_names\n",
      "table_df = table_df[1:]  # Remove the header row from the data\n",
      "\n",
      "# Step 3: Identify unique formulas and dependencies\n",
      "formulas = {}\n",
      "for col in table_df.columns:\n",
      "    if table_df[col].dtype == 'object':\n",
      "        for item in table_df[col]:\n",
      "            if isinstance(item, str) and '=' in item:\n",
      "                # Extract unique formulas\n",
      "                formulas[item] = re.findall(r'([A-Z]+[0-9]+)', item)\n",
      "\n",
      "# Step 4: Print results\n",
      "print(\"Column Names:\")\n",
      "print(column_names)\n",
      "print(\"\\nUnique Formulas and Dependencies:\")\n",
      "for formula, dependencies in formulas.items():\n",
      "    print(f\"Formula: {formula}, Dependencies: {dependencies}\")\n",
      "\n",
      "# Optionally, you can save the extracted data to a new CSV or Excel file\n",
      "# table_df.to_csv('extracted_inventory_snapshot.csv', index=False)\n",
      "```\n",
      "\n",
      "This code provides a structured way to extract and analyze the inventory snapshot data programmatically. You can modify the data input section to read from an actual file as needed.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx'\n",
    "workbook = load_workbook(file_path, data_only=False, read_only=True)\n",
    "sheet_names = [sheet_name for sheet_name in workbook.sheetnames if not workbook[sheet_name].sheet_state == 'hidden']\n",
    "\n",
    "# sheet_names = workbook.sheetnames\n",
    "\n",
    "# Function to extract the first 50 rows and all columns from a sheet\n",
    "def extract_sheet_data(sheet_name):\n",
    "    if sheet_name not in sheet_names:\n",
    "        print(f\"Sheet '{sheet_name}' does not exist in the workbook.\")\n",
    "    else:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    rows_data = []\n",
    "    for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols, values_only=False):\n",
    "        rows_data.append([cell for cell in row])\n",
    "    return rows_data\n",
    "\n",
    "# Tool to fetch sheet data\n",
    "def get_sheet_data(sheet_name: str) -> str:\n",
    "    rows = extract_sheet_data(sheet_name)\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        data.append([cell.value if cell else \"\" for cell in row])\n",
    "    return data\n",
    "\n",
    "# Tool to analyze sheet data and generate Python code\n",
    "def analyze_and_generate_code(sheet_name: str) -> str:\n",
    "    data = get_sheet_data(sheet_name)\n",
    "    # Convert data into a human-readable text format for the LLM\n",
    "    formatted_data = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator and table extraction expert.\n",
    "    Below is the first 50 rows of data from the sheet '{sheet_name}':\n",
    "\n",
    "    {formatted_data}\n",
    "\n",
    "    Your tasks:\n",
    "    1. Detect all tables in this data.\n",
    "    2. Extract table column names.\n",
    "    3. Identify unique formulas used in each column and the column dependencies in these formulas.\n",
    "    4. Generate Python code to extract the tables, their column names, and formulas programmatically.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(llm_input)\n",
    "    return response\n",
    "\n",
    "# Define Tools for the LangChain Agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"ExtractSheetData\",\n",
    "        func=lambda sheet_name: get_sheet_data(sheet_name),\n",
    "        description=\"Extracts the first 50 rows and all columns from a given sheet.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"AnalyzeSheetAndGenerateCode\",\n",
    "        func=lambda sheet_name: analyze_and_generate_code(sheet_name),\n",
    "        description=\"Analyzes a sheet and generates Python code to extract tables, column names, and formulas.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Initialize LangChain Agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use the agent to analyze all sheets\n",
    "for sheet_name in sheet_names:\n",
    "    print(f\"Analyzing sheet: {sheet_name}\")\n",
    "    response = agent.run(f\"Analyze the sheet {sheet_name} and generate code.\")\n",
    "    print(response)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sheet: INVENTORY SNAPSHOT\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to analyze the sheet named \"INVENTORY SNAPSHOT\" to understand its structure and content before generating the appropriate Python code to extract and save the tables. \n",
      "\n",
      "Action: GetSheetChunk  \n",
      "Action Input: \"INVENTORY SNAPSHOT\"  \n",
      "\u001b[0mSheet INVENTORY SNAPSHOT exists\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mNone, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, 2025-01-10 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 45 = antsy, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, SKU Description, Ref Code, Item Code, Item Code, Comarco, Lineage, TTL in storage, Dot, Total On Hand Inventory, available , None, Open PO to Dot, Open Comarco PO, None, None, None, Upcoming Dot Total, Upcoming Dot DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, FS-STICKS, 20005.0, 20005-1, -184, 14077, 13893, 8651, 22544, None, None, 3460, 0, None, FS-STICKS, 20005.0, 12111, 154.05, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, FS-NUGGS, 10150.0, 10150-1, -514, 11174, 10660, 11272, 21932, None, None, 2360, 0, None, FS-NUGGS, 10150.0, 13632, 33.2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, RTL-STICKS, 60050.0, 60050-1, 47, 4420, 4467, 6030, 10497, None, None, 4785, 0, None, RTL-STICKS, 60050.0, 10815, 45, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, RTL-NUGGS, 60056.0, 60056.0, 0, 5147, 5147, 788, 5935, None, None, 0, 0, None, RTL-NUGGS, 60056.0, 788, 40.81, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, RTL-SPICY, 60052.0, 60052.0, 1, 1404, 1405, 123, 1528, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, RTL-RANCH, 60054.0, 60054.0, -337, 1417, 1080, 386, 1466, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, FS-JR-STICKS, 40005.0, 40005-1, 38, 2152, 2190, 1072, 3262, None, None, 100, 0, None, FS-JR-STICKS, 40005.0, 1172, 31.49, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, FS-ROYALC, 37505.0, 37505.0, -32, 446, 414, 0, 414, None, None, 0, 0, None, FS-ROYALC, 37505.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, FS-FERRARO, 40055.0, 40055.0, 0, 1876, 1876, 0, 1876, None, None, 0, 0, None, FS-FERRARO, 40055.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, pallet spaces, None, None, None, -5.061965812, 281.8038462, 276.7418803, 199.3777778, 476.1196581, None, None, 72.12179487, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, PALLET CALCULATOR, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CASES, WEIGHT, None, Final DOH once delivered, None, TARGET DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, 20005.0, 20005-1, None, 0, None, 154.05, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, 10150.0, 10150-1, None, 0, x, 33.2, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, 60050.0, 60050-1, None, 0, None, 45, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, 60056.0, 60056.0, None, 0, x, 40.81, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, 60052.0, 60052.0, None, 0, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, 60054.0, 60054.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, 40005.0, 40005-1, None, 0, x, 31.49, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, 37505.0, 37505.0, 391.0, 5474, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, 40055.0, 40055.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, 0.1156804734, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, TTL PALLETS for this load:, None, None, 3.007692308, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, TTL WEIGHT for this load, 5474, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, Cases, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 2080.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 910.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, SKU Description, Item Code, Comarco, Lineage, Dot, Producing in December, None, None, Total On Hand Inventory, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Big Mozz Sticks, 20005.0, 0.0, 10618.0, 9708.0, 9500.0, None, None, 26060.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Mozz Nuggets, 10150.0, 0.0, 10429.0, 9187.0, 5200.0, None, None, 22866.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Big Mozz Sticks - Original, 60050.0, 0.0, 8039.0, -266.0, 14930.0, None, None, 12081.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Mozz Nuggets - Original, 60056.0, 0.0, 5220.0, 913.0, 3175.0, None, None, 6355.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Spicy, 60052.0, 0.0, 1530.0, 135.0, 0.0, None, None, 1686.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Ranch, 60054.0, 0.0, 1597.0, 42.0, 0.0, None, None, 1436.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP The Mozz Junior, 40005.0, 0.0, 992.0, 1403.0, 3550.0, None, None, 2632.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP 3.75\" Big Mozz Sticks, 37505.0, 0.0, 3435.0, 0.0, 0.0, None, None, 3403.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP FINALMENTE Mozz Junior, 40055.0, 0.0, 2136.0, 0.0, 0.0, None, None, 2396.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to analyze the data I received from the \"INVENTORY SNAPSHOT\" sheet to understand its structure and content better. The data appears to be a mix of inventory items with various attributes, but I need to extract a meaningful chunk to generate the appropriate Python code.\n",
      "\n",
      "Action: GetSheetChunk  \n",
      "Action Input: \"INVENTORY SNAPSHOT\"  \n",
      "\u001b[0mSheet INVENTORY SNAPSHOT exists\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mNone, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, 2025-01-10 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 45 = antsy, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, SKU Description, Ref Code, Item Code, Item Code, Comarco, Lineage, TTL in storage, Dot, Total On Hand Inventory, available , None, Open PO to Dot, Open Comarco PO, None, None, None, Upcoming Dot Total, Upcoming Dot DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, FS-STICKS, 20005.0, 20005-1, -184, 14077, 13893, 8651, 22544, None, None, 3460, 0, None, FS-STICKS, 20005.0, 12111, 154.05, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, FS-NUGGS, 10150.0, 10150-1, -514, 11174, 10660, 11272, 21932, None, None, 2360, 0, None, FS-NUGGS, 10150.0, 13632, 33.2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, RTL-STICKS, 60050.0, 60050-1, 47, 4420, 4467, 6030, 10497, None, None, 4785, 0, None, RTL-STICKS, 60050.0, 10815, 45, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, RTL-NUGGS, 60056.0, 60056.0, 0, 5147, 5147, 788, 5935, None, None, 0, 0, None, RTL-NUGGS, 60056.0, 788, 40.81, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, RTL-SPICY, 60052.0, 60052.0, 1, 1404, 1405, 123, 1528, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, RTL-RANCH, 60054.0, 60054.0, -337, 1417, 1080, 386, 1466, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, FS-JR-STICKS, 40005.0, 40005-1, 38, 2152, 2190, 1072, 3262, None, None, 100, 0, None, FS-JR-STICKS, 40005.0, 1172, 31.49, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, FS-ROYALC, 37505.0, 37505.0, -32, 446, 414, 0, 414, None, None, 0, 0, None, FS-ROYALC, 37505.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, FS-FERRARO, 40055.0, 40055.0, 0, 1876, 1876, 0, 1876, None, None, 0, 0, None, FS-FERRARO, 40055.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, pallet spaces, None, None, None, -5.061965812, 281.8038462, 276.7418803, 199.3777778, 476.1196581, None, None, 72.12179487, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, PALLET CALCULATOR, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CASES, WEIGHT, None, Final DOH once delivered, None, TARGET DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, 20005.0, 20005-1, None, 0, None, 154.05, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, 10150.0, 10150-1, None, 0, x, 33.2, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, 60050.0, 60050-1, None, 0, None, 45, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, 60056.0, 60056.0, None, 0, x, 40.81, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, 60052.0, 60052.0, None, 0, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, 60054.0, 60054.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, 40005.0, 40005-1, None, 0, x, 31.49, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, 37505.0, 37505.0, 391.0, 5474, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, 40055.0, 40055.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, 0.1156804734, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, TTL PALLETS for this load:, None, None, 3.007692308, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, TTL WEIGHT for this load, 5474, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, Cases, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 2080.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 910.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, SKU Description, Item Code, Comarco, Lineage, Dot, Producing in December, None, None, Total On Hand Inventory, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Big Mozz Sticks, 20005.0, 0.0, 10618.0, 9708.0, 9500.0, None, None, 26060.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Mozz Nuggets, 10150.0, 0.0, 10429.0, 9187.0, 5200.0, None, None, 22866.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Big Mozz Sticks - Original, 60050.0, 0.0, 8039.0, -266.0, 14930.0, None, None, 12081.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Mozz Nuggets - Original, 60056.0, 0.0, 5220.0, 913.0, 3175.0, None, None, 6355.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Spicy, 60052.0, 0.0, 1530.0, 135.0, 0.0, None, None, 1686.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Ranch, 60054.0, 0.0, 1597.0, 42.0, 0.0, None, None, 1436.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP The Mozz Junior, 40005.0, 0.0, 992.0, 1403.0, 3550.0, None, None, 2632.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP 3.75\" Big Mozz Sticks, 37505.0, 0.0, 3435.0, 0.0, 0.0, None, None, 3403.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP FINALMENTE Mozz Junior, 40055.0, 0.0, 2136.0, 0.0, 0.0, None, None, 2396.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to analyze the data I received from the \"INVENTORY SNAPSHOT\" sheet to understand its structure and content better. The data appears to be a mix of inventory items with various attributes, but I need to extract a meaningful chunk to generate the appropriate Python code.\n",
      "\n",
      "Action: AnalyzeAndGenerateCode  \n",
      "Action Input: \"INVENTORY SNAPSHOT\"  \n",
      "\u001b[0mSheet INVENTORY SNAPSHOT exists\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mcontent='To extract the tables from the provided data, we will first analyze the structure of the data to identify the tables. The data appears to contain multiple sections, with some rows containing headers and others containing data. \\n\\n### Identified Tables\\n1. **Inventory Data**: This table contains inventory details such as SKU Description, Item Code, Comarco, Lineage, and various inventory metrics.\\n2. **Pallet Calculator**: This section includes calculations related to pallets and weights.\\n3. **Final Inventory Snapshot**: This section appears to summarize the inventory with additional metrics.\\n\\n### Python Code to Extract Tables\\nBelow is the Python code that uses `pandas` to extract the identified tables and save them as separate CSV files.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\n# Sample data as a list of lists (replace this with your actual data)\\ndata = [\\n    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, None, \\'2025-01-10 00:00:00\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, \\'45 = antsy\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, \\'SKU Description\\', \\'Ref Code\\', \\'Item Code\\', \\'Item Code\\', \\'Comarco\\', \\'Lineage\\', \\'TTL in storage\\', \\'Dot\\', \\'Total On Hand Inventory\\', \\'available\\', None, \\'Open PO to Dot\\', \\'Open Comarco PO\\', None, None, None, \\'Upcoming Dot Total\\', \\'Upcoming Dot DOH\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    # ... (rest of the data)\\n]\\n\\n# Convert the data into a DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Function to extract tables based on header rows\\ndef extract_table(df, start_row, end_row):\\n    # Extract the table based on the start and end rows\\n    table = df.iloc[start_row:end_row]\\n    # Set the first row as header\\n    table.columns = table.iloc[0]\\n    table = table[1:]  # Remove the header row from the data\\n    return table.reset_index(drop=True)\\n\\n# Extracting the Inventory Data Table\\ninventory_start_row = 3\\ninventory_end_row = 14  # Adjust based on actual data\\ninventory_table = extract_table(df, inventory_start_row, inventory_end_row)\\n\\n# Extracting the Pallet Calculator Table\\npallet_calculator_start_row = 45\\npallet_calculator_end_row = 50  # Adjust based on actual data\\npallet_calculator_table = extract_table(df, pallet_calculator_start_row, pallet_calculator_end_row)\\n\\n# Extracting the Final Inventory Snapshot Table\\nfinal_inventory_start_row = 60\\nfinal_inventory_end_row = 70  # Adjust based on actual data\\nfinal_inventory_table = extract_table(df, final_inventory_start_row, final_inventory_end_row)\\n\\n# Save each table to a CSV file\\ninventory_table.to_csv(\\'inventory_data.csv\\', index=False)\\npallet_calculator_table.to_csv(\\'pallet_calculator.csv\\', index=False)\\nfinal_inventory_table.to_csv(\\'final_inventory_snapshot.csv\\', index=False)\\n\\nprint(\"Tables extracted and saved as CSV files.\")\\n```\\n\\n### Explanation of the Code\\n1. **Data Preparation**: The data is represented as a list of lists. In practice, you would replace this with the actual data you have.\\n2. **DataFrame Creation**: The data is converted into a pandas DataFrame for easier manipulation.\\n3. **Table Extraction Function**: A function `extract_table` is defined to extract tables based on specified start and end rows. It sets the first row as the header and removes it from the data.\\n4. **Extracting Tables**: The code extracts the three identified tables by specifying their start and end rows.\\n5. **Saving to CSV**: Each extracted table is saved as a separate CSV file.\\n\\n### Note\\n- Adjust the `inventory_end_row`, `pallet_calculator_end_row`, and `final_inventory_end_row` based on the actual structure of your data.\\n- Ensure that the `pandas` library is installed in your Python environment (`pip install pandas`).' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1056, 'prompt_tokens': 4529, 'total_tokens': 5585, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-8d623bea-5f93-4bc8-9287-4df3cc1295fc-0'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have successfully analyzed the \"INVENTORY SNAPSHOT\" sheet and generated the appropriate Python code to extract and save the tables. The code identifies three main tables: Inventory Data, Pallet Calculator, and Final Inventory Snapshot. \n",
      "\n",
      "Here is the generated Python code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Sample data as a list of lists (replace this with your actual data)\n",
      "data = [\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, '2025-01-10 00:00:00', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '45 = antsy', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, 'SKU Description', 'Ref Code', 'Item Code', 'Item Code', 'Comarco', 'Lineage', 'TTL in storage', 'Dot', 'Total On Hand Inventory', 'available', None, 'Open PO to Dot', 'Open Comarco PO', None, None, None, 'Upcoming Dot Total', 'Upcoming Dot DOH', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    # ... (rest of the data)\n",
      "]\n",
      "\n",
      "# Convert the data into a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Function to extract tables based on header rows\n",
      "def extract_table(df, start_row, end_row):\n",
      "    # Extract the table based on the start and end rows\n",
      "    table = df.iloc[start_row:end_row]\n",
      "    # Set the first row as header\n",
      "    table.columns = table.iloc[0]\n",
      "    table = table[1:]  # Remove the header row from the data\n",
      "    return table.reset_index(drop=True)\n",
      "\n",
      "# Extracting the Inventory Data Table\n",
      "inventory_start_row = 3\n",
      "inventory_end_row = 14  # Adjust based on actual data\n",
      "inventory_table = extract_table(df, inventory_start_row, inventory_end_row)\n",
      "\n",
      "# Extracting the Pallet Calculator Table\n",
      "pallet_calculator_start_row = 45\n",
      "pallet_calculator_end_row = 50  # Adjust based on actual data\n",
      "pallet_calculator_table = extract_table(df, pallet_calculator_start_row, pallet_calculator_end_row)\n",
      "\n",
      "# Extracting the Final Inventory Snapshot Table\n",
      "final_inventory_start_row = 60\n",
      "final_inventory_end_row = 70  # Adjust based on actual data\n",
      "final_inventory_table = extract_table(df, final_inventory_start_row, final_inventory_end_row)\n",
      "\n",
      "# Save each table to a CSV file\n",
      "inventory_table.to_csv('inventory_data.csv', index=False)\n",
      "pallet_calculator_table.to_csv('pallet_calculator.csv', index=False)\n",
      "final_inventory_table.to_csv('final_inventory_snapshot.csv', index=False)\n",
      "\n",
      "print(\"Tables extracted and saved as CSV files.\")\n",
      "```\n",
      "\n",
      "### Explanation of the Code\n",
      "1. **Data Preparation**: The data is represented as a list of lists. In practice, you would replace this with the actual data you have.\n",
      "2. **DataFrame Creation**: The data is converted into a pandas DataFrame for easier manipulation.\n",
      "3. **Table Extraction Function**: A function `extract_table` is defined to extract tables based on specified start and end rows. It sets the first row as the header and removes it from the data.\n",
      "4. **Extracting Tables**: The code extracts the three identified tables by specifying their start and end rows.\n",
      "5. **Saving to CSV**: Each extracted table is saved as a separate CSV file.\n",
      "\n",
      "### Note\n",
      "- Adjust the `inventory_end_row`, `pallet_calculator_end_row`, and `final_inventory_end_row` based on the actual structure of your data.\n",
      "- Ensure that the `pandas` library is installed in your Python environment (`pip install pandas`).\n",
      "\n",
      "Final Answer: The Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet has been generated successfully.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet has been generated successfully.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to execute the generated Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet. However, I don't have the actual code yet. I should first fetch a chunk of data from the \"INVENTORY SNAPSHOT\" sheet to analyze it and generate the necessary code.\n",
      "\n",
      "Action: GetSheetChunk  \n",
      "Action Input: \"INVENTORY SNAPSHOT\"  \n",
      "\u001b[0mSheet INVENTORY SNAPSHOT exists\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mNone, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, 2025-01-10 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 45 = antsy, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, SKU Description, Ref Code, Item Code, Item Code, Comarco, Lineage, TTL in storage, Dot, Total On Hand Inventory, available , None, Open PO to Dot, Open Comarco PO, None, None, None, Upcoming Dot Total, Upcoming Dot DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, FS-STICKS, 20005.0, 20005-1, -184, 14077, 13893, 8651, 22544, None, None, 3460, 0, None, FS-STICKS, 20005.0, 12111, 154.05, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, FS-NUGGS, 10150.0, 10150-1, -514, 11174, 10660, 11272, 21932, None, None, 2360, 0, None, FS-NUGGS, 10150.0, 13632, 33.2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, RTL-STICKS, 60050.0, 60050-1, 47, 4420, 4467, 6030, 10497, None, None, 4785, 0, None, RTL-STICKS, 60050.0, 10815, 45, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, RTL-NUGGS, 60056.0, 60056.0, 0, 5147, 5147, 788, 5935, None, None, 0, 0, None, RTL-NUGGS, 60056.0, 788, 40.81, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, RTL-SPICY, 60052.0, 60052.0, 1, 1404, 1405, 123, 1528, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, RTL-RANCH, 60054.0, 60054.0, -337, 1417, 1080, 386, 1466, None, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, FS-JR-STICKS, 40005.0, 40005-1, 38, 2152, 2190, 1072, 3262, None, None, 100, 0, None, FS-JR-STICKS, 40005.0, 1172, 31.49, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, FS-ROYALC, 37505.0, 37505.0, -32, 446, 414, 0, 414, None, None, 0, 0, None, FS-ROYALC, 37505.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, FS-FERRARO, 40055.0, 40055.0, 0, 1876, 1876, 0, 1876, None, None, 0, 0, None, FS-FERRARO, 40055.0, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, pallet spaces, None, None, None, -5.061965812, 281.8038462, 276.7418803, 199.3777778, 476.1196581, None, None, 72.12179487, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, PALLET CALCULATOR, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CASES, WEIGHT, None, Final DOH once delivered, None, TARGET DOH, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Big Mozz Sticks, 20005.0, 20005-1, None, 0, None, 154.05, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Mozz Nuggets, 10150.0, 10150-1, None, 0, x, 33.2, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Big Mozz Sticks - Original, 60050.0, 60050-1, None, 0, None, 45, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP Retail Mozz Nuggets - Original, 60056.0, 60056.0, None, 0, x, 40.81, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Spicy, 60052.0, 60052.0, None, 0, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, Retail Ranch, 60054.0, 60054.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP The Mozz Junior, 40005.0, 40005-1, None, 0, x, 31.49, None, 60.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, 37505.0, 37505.0, 391.0, 5474, x, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, 40055.0, 40055.0, None, 0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, 0.1156804734, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, TTL PALLETS for this load:, None, None, 3.007692308, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, TTL WEIGHT for this load, 5474, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, Cases, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 2080.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, 910.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, SKU Description, Item Code, Comarco, Lineage, Dot, Producing in December, None, None, Total On Hand Inventory, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Big Mozz Sticks, 20005.0, 0.0, 10618.0, 9708.0, 9500.0, None, None, 26060.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Mozz Nuggets, 10150.0, 0.0, 10429.0, 9187.0, 5200.0, None, None, 22866.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Big Mozz Sticks - Original, 60050.0, 0.0, 8039.0, -266.0, 14930.0, None, None, 12081.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP Retail Mozz Nuggets - Original, 60056.0, 0.0, 5220.0, 913.0, 3175.0, None, None, 6355.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Spicy, 60052.0, 0.0, 1530.0, 135.0, 0.0, None, None, 1686.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, Retail Ranch, 60054.0, 0.0, 1597.0, 42.0, 0.0, None, None, 1436.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP The Mozz Junior, 40005.0, 0.0, 992.0, 1403.0, 3550.0, None, None, 2632.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP 3.75\" Big Mozz Sticks, 37505.0, 0.0, 3435.0, 0.0, 0.0, None, None, 3403.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, CP FINALMENTE Mozz Junior, 40055.0, 0.0, 2136.0, 0.0, 0.0, None, None, 2396.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have fetched a chunk of data from the \"INVENTORY SNAPSHOT\" sheet. The data includes various columns such as SKU Description, Item Code, Comarco, Lineage, and Total On Hand Inventory, among others. \n",
      "\n",
      "Next, I will analyze this data to generate the necessary Python code for extracting and saving the tables.\n",
      "\n",
      "Action: AnalyzeAndGenerateCode  \n",
      "Action Input: \"INVENTORY SNAPSHOT\"  \n",
      "\u001b[0mSheet INVENTORY SNAPSHOT exists\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mcontent='To extract the tables from the provided data, we will analyze the structure of the data to identify the tables. The data appears to contain multiple sections, with some rows containing headers and others containing data. \\n\\n### Identified Tables\\n1. **Inventory Data Table**: This table contains inventory details such as SKU Description, Item Code, Comarco, Lineage, and various inventory metrics.\\n2. **Pallet Calculator Table**: This table contains information related to pallet calculations, including cases and weight.\\n3. **Production Data Table**: This table contains production-related data for various items.\\n\\n### Python Code to Extract Tables\\nBelow is the Python code that uses `pandas` to extract the identified tables into separate DataFrames and save them as CSV files.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\n# Raw data as a list of lists (replace this with your actual data)\\ndata = [\\n    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, None, \\'2025-01-10 00:00:00\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, \\'45 = antsy\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    [None, \\'SKU Description\\', \\'Ref Code\\', \\'Item Code\\', \\'Item Code\\', \\'Comarco\\', \\'Lineage\\', \\'TTL in storage\\', \\'Dot\\', \\'Total On Hand Inventory\\', \\'available\\', None, \\'Open PO to Dot\\', \\'Open Comarco PO\\', None, None, None, \\'Upcoming Dot Total\\', \\'Upcoming Dot DOH\\', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\\n    # ... (rest of the data)\\n]\\n\\n# Convert the raw data into a DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Extract Inventory Data Table\\ninventory_start_row = df[df[0] == \\'SKU Description\\'].index[0] + 1\\ninventory_end_row = df[df[0] == \\'pallet spaces\\'].index[0]\\ninventory_data = df.iloc[inventory_start_row:inventory_end_row]\\ninventory_data.columns = df.iloc[inventory_start_row - 1]\\ninventory_data.reset_index(drop=True, inplace=True)\\n\\n# Save Inventory Data to CSV\\ninventory_data.to_csv(\\'inventory_data.csv\\', index=False)\\n\\n# Extract Pallet Calculator Table\\npallet_calculator_start_row = df[df[0] == \\'pallet spaces\\'].index[0] + 1\\npallet_calculator_end_row = df[df[0] == \\'TTL PALLETS for this load:\\'].index[0]\\npallet_calculator_data = df.iloc[pallet_calculator_start_row:pallet_calculator_end_row]\\npallet_calculator_data.columns = df.iloc[pallet_calculator_start_row - 1]\\npallet_calculator_data.reset_index(drop=True, inplace=True)\\n\\n# Save Pallet Calculator Data to CSV\\npallet_calculator_data.to_csv(\\'pallet_calculator_data.csv\\', index=False)\\n\\n# Extract Production Data Table\\nproduction_start_row = df[df[0] == \\'SKU Description\\'].index[0] + 1\\nproduction_end_row = df[df[0] == \\'CP Big Mozz Sticks\\'].index[0] + 1\\nproduction_data = df.iloc[production_start_row:production_end_row]\\nproduction_data.columns = df.iloc[production_start_row - 1]\\nproduction_data.reset_index(drop=True, inplace=True)\\n\\n# Save Production Data to CSV\\nproduction_data.to_csv(\\'production_data.csv\\', index=False)\\n\\nprint(\"Data extraction complete. CSV files saved.\")\\n```\\n\\n### Explanation of the Code\\n1. **Data Preparation**: The raw data is represented as a list of lists. You should replace this with your actual data.\\n2. **DataFrame Creation**: The data is converted into a pandas DataFrame for easier manipulation.\\n3. **Table Extraction**:\\n   - The code identifies the start and end rows for each table based on specific header rows.\\n   - It slices the DataFrame to extract the relevant data and sets the appropriate column names.\\n4. **CSV Saving**: Each extracted table is saved as a separate CSV file.\\n\\n### Note\\nMake sure to adjust the indices and conditions based on the actual structure of your data if necessary. The provided code assumes a specific structure based on the sample data you provided.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 1085, 'prompt_tokens': 4529, 'total_tokens': 5614, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4352}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-bcd4e8de-5d2d-491f-b8af-2b090b3a2d4a-0'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have successfully generated the Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet. The code identifies and extracts three main tables: the Inventory Data Table, the Pallet Calculator Table, and the Production Data Table. Each table is saved as a separate CSV file.\n",
      "\n",
      "Here is the generated Python code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Raw data as a list of lists (replace this with your actual data)\n",
      "data = [\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, '2025-01-10 00:00:00', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '45 = antsy', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    [None, 'SKU Description', 'Ref Code', 'Item Code', 'Item Code', 'Comarco', 'Lineage', 'TTL in storage', 'Dot', 'Total On Hand Inventory', 'available', None, 'Open PO to Dot', 'Open Comarco PO', None, None, None, 'Upcoming Dot Total', 'Upcoming Dot DOH', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\n",
      "    # ... (rest of the data)\n",
      "]\n",
      "\n",
      "# Convert the raw data into a DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Extract Inventory Data Table\n",
      "inventory_start_row = df[df[0] == 'SKU Description'].index[0] + 1\n",
      "inventory_end_row = df[df[0] == 'pallet spaces'].index[0]\n",
      "inventory_data = df.iloc[inventory_start_row:inventory_end_row]\n",
      "inventory_data.columns = df.iloc[inventory_start_row - 1]\n",
      "inventory_data.reset_index(drop=True, inplace=True)\n",
      "\n",
      "# Save Inventory Data to CSV\n",
      "inventory_data.to_csv('inventory_data.csv', index=False)\n",
      "\n",
      "# Extract Pallet Calculator Table\n",
      "pallet_calculator_start_row = df[df[0] == 'pallet spaces'].index[0] + 1\n",
      "pallet_calculator_end_row = df[df[0] == 'TTL PALLETS for this load:'].index[0]\n",
      "pallet_calculator_data = df.iloc[pallet_calculator_start_row:pallet_calculator_end_row]\n",
      "pallet_calculator_data.columns = df.iloc[pallet_calculator_start_row - 1]\n",
      "pallet_calculator_data.reset_index(drop=True, inplace=True)\n",
      "\n",
      "# Save Pallet Calculator Data to CSV\n",
      "pallet_calculator_data.to_csv('pallet_calculator_data.csv', index=False)\n",
      "\n",
      "# Extract Production Data Table\n",
      "production_start_row = df[df[0] == 'SKU Description'].index[0] + 1\n",
      "production_end_row = df[df[0] == 'CP Big Mozz Sticks'].index[0] + 1\n",
      "production_data = df.iloc[production_start_row:production_end_row]\n",
      "production_data.columns = df.iloc[production_start_row - 1]\n",
      "production_data.reset_index(drop=True, inplace=True)\n",
      "\n",
      "# Save Production Data to CSV\n",
      "production_data.to_csv('production_data.csv', index=False)\n",
      "\n",
      "print(\"Data extraction complete. CSV files saved.\")\n",
      "```\n",
      "\n",
      "### Explanation of the Code\n",
      "1. **Data Preparation**: The raw data is represented as a list of lists. You should replace this with your actual data.\n",
      "2. **DataFrame Creation**: The data is converted into a pandas DataFrame for easier manipulation.\n",
      "3. **Table Extraction**:\n",
      "   - The code identifies the start and end rows for each table based on specific header rows.\n",
      "   - It slices the DataFrame to extract the relevant data and sets the appropriate column names.\n",
      "4. **CSV Saving**: Each extracted table is saved as a separate CSV file.\n",
      "\n",
      "### Note\n",
      "Make sure to adjust the indices and conditions based on the actual structure of your data if necessary. The provided code assumes a specific structure based on the sample data you provided.\n",
      "\n",
      "Final Answer: The Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet has been generated successfully.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Python code to extract and save tables from the \"INVENTORY SNAPSHOT\" sheet has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = \"/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx\"  # Replace with your file path\n",
    "workbook = load_workbook(file_path, data_only=True, read_only=True)\n",
    "sheet_names = workbook.sheetnames\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name in sheet_names:\n",
    "        print(f\"Sheet {sheet_name.strip()} exists\")\n",
    "    else:\n",
    "        print(f\"Sheet {sheet_name.strip()} doesn't exist\")\n",
    "        return False\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    return data\n",
    "\n",
    "# Tool: Fetch the sheet chunk\n",
    "def get_sheet_chunk(sheet_name: str) -> str:\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in chunk])\n",
    "    return formatted_chunk\n",
    "\n",
    "# Tool: Analyze and generate code for extracting tables\n",
    "def analyze_and_generate_code(sheet_name: str) -> str:\n",
    "    chunk = get_sheet_chunk(sheet_name)\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}':\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present.\n",
    "    2. Generate Python code to extract each table programmatically.\n",
    "    3. The code should extract each table into a pandas DataFrame and save it as a separate CSV file.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(llm_input)\n",
    "    return response\n",
    "\n",
    "# Tool Definitions for LangChain\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"GetSheetChunk\",\n",
    "        func=lambda sheet_name: get_sheet_chunk(sheet_name),\n",
    "        description=\"Fetches a 50xN chunk of data from the specified sheet.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"AnalyzeAndGenerateCode\",\n",
    "        func=lambda sheet_name: analyze_and_generate_code(sheet_name),\n",
    "        description=\"Analyzes a sheet chunk and generates Python code to extract and save tables.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"PythonREPL\",\n",
    "        func=PythonREPLTool().run,\n",
    "        description=\"Executes Python code in a REPL environment.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Initialize LangChain Agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the agent on a specific sheet\n",
    "sheet_to_analyze = \"INVENTORY SNAPSHOT\"  # Replace with your target sheet\n",
    "try:\n",
    "    print(f\"Analyzing sheet: {sheet_to_analyze}\")\n",
    "    response = agent.run(f\"Analyze the sheet {sheet_to_analyze} and generate code.\")\n",
    "    print(response)\n",
    "\n",
    "    # Execute the generated code\n",
    "    execution_response = agent.run(f\"Execute the following Python code: {response}\")\n",
    "    print(execution_response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for sheet: Current Inventory\n",
      "Fetching 50xN chunk from sheet: Current Inventory\n",
      "Fetched chunk from sheet 'Current Inventory':\n",
      "INVENTORY BY LOCATION, None, None, None, None, INVENTORY TOTALS -- EXCLUDING EGS, None, None, None\n",
      "None, None, None, None, None, None, None, None, None\n",
      "Date, 2025-01-05 00:00:00, None, None, None, Date, 2024-11-14 00:00:00, None, None\n",
      "None, None, None, None, None, None, None, None\n",
      "Location, Description, Code, Case Quantity per Tracker AS OF DATE, None, Description, Code, Case Quantity per Tracker AS OF DATE\n",
      "Comarco Products, CP Big Mozz Sticks, 20005-1, -184, None, CP Big Mozz Sticks, 20005-1, 1...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that uses pandas to read the Excel file, identify the tables, and save them as separate CSV files:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory', header=None)\n",
      "\n",
      "# Identify the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if pd.isnull(df.iloc[i, 0]) and len(current_table) > 0:\n",
      "        tables.append(pd.Dat...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory', header=None)\n",
      "\n",
      "# Identify the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if pd.isnull(df.iloc[i, 0]) and len(current_table) > 0:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "        current_table = []\n",
      "    else:\n",
      "        current_table.append(df.iloc[i].tolist())\n",
      "if len(current_table) > ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory', header=None)\n",
      "\n",
      "# Identify the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if pd.isnull(df.iloc[i, 0]) and len(current_table) > 0:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "        current_table = []\n",
      "    else:\n",
      "        current_table.append(df.iloc[i].tolist())\n",
      "if len(current_table) > ...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = \"/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx\"  # Replace with actual file path\n",
    "workbook = load_workbook(file_path, data_only=True, read_only=True)\n",
    "sheet_names = workbook.sheetnames\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name not in sheet_names:\n",
    "        log_message = f\"Sheet '{sheet_name}' doesn't exist. Available sheets: {sheet_names}\"\n",
    "        print(log_message)\n",
    "        return log_message\n",
    "    print(f\"Fetching 50xN chunk from sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    print(f\"Fetched chunk from sheet '{sheet_name}':\\n{formatted_chunk[:500]}...\")  # Show first 500 chars for brevity\n",
    "    return formatted_chunk\n",
    "\n",
    "# Function to analyze a chunk and generate code\n",
    "import re\n",
    "\n",
    "def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present.\n",
    "    2. Generate Python code that reads data from the provided Excel file ('{file_path}') and the specified sheet ('{sheet_name}').\n",
    "    3. Extract each identified table and save it as a separate CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    try:\n",
    "        response = llm.invoke(llm_input)\n",
    "        generated_response = response.content  # Extract string content\n",
    "        print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "        # Extract Python code block using regex\n",
    "        match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "        if match:\n",
    "            generated_code = match.group(1).strip()  # Extract the Python code\n",
    "            print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "            return generated_code\n",
    "        else:\n",
    "            print(\"No valid Python code block found in the LLM response.\")\n",
    "            return \"Error: No valid Python code block found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code generation: {e}\")\n",
    "        return f\"Error during code generation: {e}\"\n",
    "\n",
    "# Function to execute generated Python code\n",
    "def execute_code(code: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executing the following code:\\n{code[:500]}...\")  # Log the extracted code\n",
    "        repl_tool = PythonREPLTool()\n",
    "        result = repl_tool.run(code)\n",
    "        print(\"Execution result:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"Execution Error: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return error_message\n",
    "\n",
    "# Main execution process\n",
    "def run_analysis(sheet_name):\n",
    "    print(f\"Starting analysis for sheet: {sheet_name}\")\n",
    "    \n",
    "    # Step 1: Fetch sheet chunk\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    if \"doesn't exist\" in chunk:\n",
    "        print(f\"Error: {chunk}\")\n",
    "        return chunk\n",
    "\n",
    "    # Step 2: Analyze and generate code\n",
    "    print(\"Analyzing the data and generating Python code...\")\n",
    "    code = analyze_and_generate_code(sheet_name, chunk)\n",
    "    if \"Error during code generation\" in code:\n",
    "        return code\n",
    "\n",
    "    # Step 3: Attempt to execute code with retries\n",
    "    retry = 10\n",
    "    while retry > 0:\n",
    "        result = execute_code(code)\n",
    "        if \"Execution Error\" not in result:\n",
    "            print(\"Code executed successfully!\")\n",
    "            return result  # Successful execution\n",
    "\n",
    "        # Provide feedback and request fixes\n",
    "        error_message = f\"\\n\\nError Message: {result}\"\n",
    "        print(f\"Retrying... ({10 - retry + 1}/10)\")\n",
    "        print(f\"Requesting LLM to fix the code. Error: {error_message}\")\n",
    "        code = analyze_and_generate_code(sheet_name, chunk + error_message)\n",
    "        retry -= 1\n",
    "\n",
    "    print(\"Exceeded maximum retries. Could not execute code successfully.\")\n",
    "    return \"Failed after multiple retries.\"\n",
    "\n",
    "# Run the workflow\n",
    "sheet_to_analyze = \"Current Inventory\"  # Replace with your target sheet name\n",
    "print(run_analysis(sheet_to_analyze))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if 'table_' in filename:\n",
    "        print(filename)\n",
    "        display(pd.read_csv(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for sheet: True Up Template\n",
      "Fetching 50xN chunk from sheet: True Up Template\n",
      "Fetched chunk from sheet 'True Up Template':\n",
      "Date, 2024-10-03 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, COUNTS FROM WAREHOUSES, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "Location, Description, Code, Case Quantity per Tracker AS OF DATE, Actual Quantity AS OF DATE, Difference, None, None, None, None, None, Location, Description, Jan 11th 2024, Jan 21st 2024, 2024-02-01 00:00:00, Mid-Fe...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code to perform the tasks:\n",
      "\n",
      "```python\n",
      "import openpyxl\n",
      "from openpyxl.utils import get_column_letter, column_index_from_string\n",
      "\n",
      "def extract_formulas(file_path, sheet_name):\n",
      "    # Load workbook with data_only=False to fetch formulas\n",
      "    wb_formulas = openpyxl.load_workbook(file_path, data_only=False)\n",
      "    sheet_formulas = wb_formulas[sheet_name]\n",
      "\n",
      "    # Load workbook with data_only=True to fetch column names and values\n",
      "    wb_values = openpyxl.load_workbook(file_path, data_only=Tru...\n",
      "Extracted code:\n",
      "import openpyxl\n",
      "from openpyxl.utils import get_column_letter, column_index_from_string\n",
      "\n",
      "def extract_formulas(file_path, sheet_name):\n",
      "    # Load workbook with data_only=False to fetch formulas\n",
      "    wb_formulas = openpyxl.load_workbook(file_path, data_only=False)\n",
      "    sheet_formulas = wb_formulas[sheet_name]\n",
      "\n",
      "    # Load workbook with data_only=True to fetch column names and values\n",
      "    wb_values = openpyxl.load_workbook(file_path, data_only=True)\n",
      "    sheet_values = wb_values[sheet_name]\n",
      "\n",
      "    # Get co...\n",
      "Executing the following code:\n",
      "import openpyxl\n",
      "from openpyxl.utils import get_column_letter, column_index_from_string\n",
      "\n",
      "def extract_formulas(file_path, sheet_name):\n",
      "    # Load workbook with data_only=False to fetch formulas\n",
      "    wb_formulas = openpyxl.load_workbook(file_path, data_only=False)\n",
      "    sheet_formulas = wb_formulas[sheet_name]\n",
      "\n",
      "    # Load workbook with data_only=True to fetch column names and values\n",
      "    wb_values = openpyxl.load_workbook(file_path, data_only=True)\n",
      "    sheet_values = wb_values[sheet_name]\n",
      "\n",
      "    # Get column names from the first row of the sheet\n",
      "    column_names = [cell.value for cell in sheet_values[1]]\n",
      "\n",
      "    # Initialize dictionary to store column names and their formulas\n",
      "    column_formulas = {}\n",
      "\n",
      "    # Iterate over each column\n",
      "    for col_idx, col_name in enumerate(column_names, start=1):\n",
      "        # Get column letter\n",
      "        col_letter = get_column_letter(col_idx)\n",
      "\n",
      "        # Initialize formula for the column\n",
      "        formula = None\n",
      "\n",
      "        # Iterate over each cell in the column\n",
      "        for row_idx in range(2, sheet_formulas.max_row + 1):\n",
      "            cell = sheet_formulas['{}{}'.format(col_letter, row_idx)]\n",
      "\n",
      "            # If cell contains a formula, store it\n",
      "            if cell.data_type == 'f':\n",
      "                formula = cell.value\n",
      "\n",
      "                # Replace cell references in the formula with their column names\n",
      "                for ref_col_idx in range(1, sheet_formulas.max_column + 1):\n",
      "                    ref_col_letter = get_column_letter(ref_col_idx)\n",
      "                    ref_col_name = column_names[ref_col_idx - 1]\n",
      "                    formula = formula.replace(ref_col_letter, '{}({})'.format(ref_col_letter, ref_col_name))\n",
      "\n",
      "                # Break the loop as we found a formula\n",
      "                break\n",
      "\n",
      "        # Store the formula for the column\n",
      "        column_formulas[col_name] = formula\n",
      "\n",
      "    return column_formulas\n",
      "\n",
      "# Test the function\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx'\n",
      "sheet_name = 'True Up Template'\n",
      "print(extract_formulas(file_path, sheet_name))...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition1.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition2.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition3.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition4.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition5.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition6.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition7.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition8.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition9.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition10.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition11.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition12.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition13.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition14.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/packaging/relationship.py:118: UserWarning: xl/pivotCache/_rels/pivotCacheDefinition15.xml.rels contains invalid dependency definitions\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution result:\n",
      "{'Date': None, datetime.datetime(2024, 10, 3, 0, 0): None, None: None}\n",
      "\n",
      "Code executed successfully!\n",
      "{'Date': None, datetime.datetime(2024, 10, 3, 0, 0): None, None: None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = \"/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx\"  # Replace with actual file path\n",
    "workbook = load_workbook(file_path, data_only=True, read_only=True)\n",
    "sheet_names = workbook.sheetnames\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name not in sheet_names:\n",
    "        log_message = f\"Sheet '{sheet_name}' doesn't exist. Available sheets: {sheet_names}\"\n",
    "        print(log_message)\n",
    "        return log_message\n",
    "    print(f\"Fetching 50xN chunk from sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    print(f\"Fetched chunk from sheet '{sheet_name}':\\n{formatted_chunk[:500]}...\")  # Show first 500 chars for brevity\n",
    "    return formatted_chunk\n",
    "\n",
    "# Function to analyze a chunk and generate code\n",
    "import re\n",
    "\n",
    "# def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "#     llm_input = f\"\"\"\n",
    "#     You are a Python code generator for table extraction.\n",
    "#     Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "#     {chunk}\n",
    "\n",
    "#     Tasks:\n",
    "#     1. Analyze the data to identify all tables present.\n",
    "#     2. Generate Python code that reads data from the provided Excel file ('{file_path}') and the specified sheet ('{sheet_name}').\n",
    "#     3. Create a new folder named '{sheet_name}' in the current working directory.\n",
    "#     4. Extract each identified table and save it as a separate CSV file in the folder '{sheet_name}'.\n",
    "#     \"\"\"\n",
    "#     print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "#     llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "#     try:\n",
    "#         response = llm.invoke(llm_input)\n",
    "#         generated_response = response.content  # Extract string content\n",
    "#         print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "#         # Extract Python code block using regex\n",
    "#         match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "#         if match:\n",
    "#             generated_code = match.group(1).strip()  # Extract the Python code\n",
    "#             print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "#             return generated_code\n",
    "#         else:\n",
    "#             print(\"No valid Python code block found in the LLM response.\")\n",
    "#             return \"Error: No valid Python code block found.\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during code generation: {e}\")\n",
    "#         return f\"Error during code generation: {e}\"\n",
    "\n",
    "# best working - Only gets all the csvs. Doesn't get formulas yet\n",
    "def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present in the chunk. Tables are defined as contiguous blocks of data separated by blank rows or rows with no data.\n",
    "    2. Generate Python code that:\n",
    "       - Reads data from the provided Excel file ('{file_path}') and specified sheet ('{sheet_name}').\n",
    "       - Creates a new folder named '{sheet_name}'.\n",
    "       - Extracts each identified table based on the above definition and saves it as a separate CSV file in the folder.\n",
    "    3. Do not make any assumptions about the meaning or functionality of specific columns unless explicitly mentioned.\n",
    "    \"\"\"\n",
    "    print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    try:\n",
    "        response = llm.invoke(llm_input)\n",
    "        generated_response = response.content  # Extract string content\n",
    "        print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "        # Extract Python code block using regex\n",
    "        match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "        if match:\n",
    "            generated_code = match.group(1).strip()  # Extract the Python code\n",
    "            print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "            return generated_code\n",
    "        else:\n",
    "            print(\"No valid Python code block found in the LLM response.\")\n",
    "            return \"Error: No valid Python code block found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code generation: {e}\")\n",
    "        return f\"Error during code generation: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to execute generated Python code\n",
    "def execute_code(code: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executing the following code:\\n{code}...\")  # Log the extracted code\n",
    "        repl_tool = PythonREPLTool()\n",
    "        result = repl_tool.run(code)\n",
    "        print(\"Execution result:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"Execution Error: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return error_message\n",
    "\n",
    "# Main execution process\n",
    "def run_analysis(sheet_name):\n",
    "    print(f\"Starting analysis for sheet: {sheet_name}\")\n",
    "    \n",
    "    # Step 1: Fetch sheet chunk\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    if \"doesn't exist\" in chunk:\n",
    "        print(f\"Error: {chunk}\")\n",
    "        return chunk\n",
    "\n",
    "    # Step 2: Analyze and generate code\n",
    "    print(\"Analyzing the data and generating Python code...\")\n",
    "    code = analyze_and_generate_code(sheet_name, chunk)\n",
    "    if \"Error during code generation\" in code:\n",
    "        return code\n",
    "\n",
    "    # Step 3: Attempt to execute code with retries\n",
    "    retry = 10\n",
    "    while retry > 0:\n",
    "        result = execute_code(code)\n",
    "        if \"Execution Error\" not in result:\n",
    "            print(\"Code executed successfully!\")\n",
    "            return result  # Successful execution\n",
    "\n",
    "        # Provide feedback and request fixes\n",
    "        error_message = f\"\\n\\nError Message: {result}\"\n",
    "        print(f\"Retrying... ({10 - retry + 1}/10)\")\n",
    "        print(f\"Requesting LLM to fix the code. Error: {error_message}\")\n",
    "        code = analyze_and_generate_code(sheet_name, chunk + error_message)\n",
    "        retry -= 1\n",
    "\n",
    "    print(\"Exceeded maximum retries. Could not execute code successfully.\")\n",
    "    return \"Failed after multiple retries.\"\n",
    "\n",
    "# Run the workflow\n",
    "sheet_to_analyze = \"True Up Template\"  # Replace with your target sheet name\n",
    "print(run_analysis(sheet_to_analyze))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "for filename in os.listdir(sheet_to_analyze):\n",
    "    if '.csv' in filename:\n",
    "        print(filename)\n",
    "        display(pd.read_csv(os.path.join(os.getcwd(),sheet_to_analyze,filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a table is related to inventory planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_2.csv\n",
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning, such as SKU, location, or inventory quantities.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"Product Description\",\n",
      "        \"Location/Warehouse\": \"Location\",\n",
      "        \"Quantity\": \"Cases\",\n",
      "        \"Total Inventory\": \"Unnamed: 12\",\n",
      "        \"Current Inventory\": \"Unnamed: 13\",\n",
      "        \"Sales Forecast\": \"Unnamed: 14\"\n",
      "    },\n",
      "    \"description\": \"This sheet contains data related to product descriptions, quantities, and locations, which are essential for inventory planning and management.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"Unnamed: 3\",\n",
      "        \"Location/Warehouse\": \"Date\",\n",
      "        \"Quantity\": \"Unnamed: 4\",\n",
      "        \"Total Inventory\": \"Unnamed: 6\",\n",
      "        \"Current Inventory\": \"Unnamed: 5\",\n",
      "        \"Sales Forecast\": \"Unnamed: 10\"\n",
      "    },\n",
      "    \"description\": \"This sheet is used for inventory planning as it contains information about product SKUs, their quantities, and inventory levels, which are essential for managing stock and forecasting sales.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"Unnamed: 1\",\n",
      "        \"Location/Warehouse\": \"Unnamed: 6\",\n",
      "        \"Quantity\": \"Unnamed: 9\",\n",
      "        \"Total Inventory\": \"Unnamed: 10\",\n",
      "        \"Current Inventory\": \"Unnamed: 8\",\n",
      "        \"Sales Forecast\": \"Unnamed: 18\"\n",
      "    },\n",
      "    \"description\": \"This sheet contains information about SKU descriptions, inventory levels, and upcoming demand, which are essential for effective inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"None\",\n",
      "        \"Location/Warehouse\": \"None\",\n",
      "        \"Quantity\": \"Unnamed: 5\",\n",
      "        \"Total Inventory\": \"Unnamed: 1\",\n",
      "        \"Current Inventory\": \"Unnamed: 6\",\n",
      "        \"Sales Forecast\": \"Unnamed: 12\"\n",
      "    },\n",
      "    \"description\": \"This sheet appears to be used for inventory planning as it includes columns that could represent quantities and inventory levels, which are essential for managing stock and forecasting sales.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"The provided data does not contain any relevant columns or information typically associated with inventory planning, such as SKU, quantities, or inventory levels.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"Unnamed: 2\",\n",
      "        \"Location/Warehouse\": \"Unnamed: 3\",\n",
      "        \"Quantity\": \"Unnamed: 5\",\n",
      "        \"Total Inventory\": \"Unnamed: 8\",\n",
      "        \"Current Inventory\": \"Unnamed: 9\",\n",
      "        \"Sales Forecast\": \"Unnamed: 10\"\n",
      "    },\n",
      "    \"description\": \"This sheet contains product SKUs, their locations, quantities, and other relevant metrics that are essential for managing inventory levels and planning for stock replenishment.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"None\",\n",
      "        \"Location/Warehouse\": \"None\",\n",
      "        \"Quantity\": \"Unnamed: 5\",\n",
      "        \"Total Inventory\": \"None\",\n",
      "        \"Current Inventory\": \"None\",\n",
      "        \"Sales Forecast\": \"None\"\n",
      "    },\n",
      "    \"description\": \"This sheet contains information about pallets and weight for a load, which are relevant for inventory management and planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"Unnamed: 5\",\n",
      "        \"Location/Warehouse\": \"Unnamed: 6\",\n",
      "        \"Quantity\": \"Unnamed: 8\",\n",
      "        \"Total Inventory\": \"Unnamed: 12\",\n",
      "        \"Current Inventory\": \"Unnamed: 9\",\n",
      "        \"Sales Forecast\": \"Unnamed: 10\"\n",
      "    },\n",
      "    \"description\": \"This sheet contains information about SKU descriptions, item codes, inventory levels, and sales forecasts, which are essential for effective inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"None\",\n",
      "        \"Location/Warehouse\": \"None\",\n",
      "        \"Quantity\": \"Unnamed: 3\",\n",
      "        \"Total Inventory\": \"None\",\n",
      "        \"Current Inventory\": \"None\",\n",
      "        \"Sales Forecast\": \"None\"\n",
      "    },\n",
      "    \"description\": \"This sheet appears to track cases, which suggests it is related to inventory management, specifically monitoring the number of cases available.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_44.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_78.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_93.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_123.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_137.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_87.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_136.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_86.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_92.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_122.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_79.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"The sheet appears to be a header or title row without any actual data or relevant columns for inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_45.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 1,\n",
      "    \"details\": {\n",
      "        \"SKU\": \"None\",\n",
      "        \"Location/Warehouse\": \"None\",\n",
      "        \"Quantity\": \"None\",\n",
      "        \"Total Inventory\": \"None\",\n",
      "        \"Current Inventory\": \"None\",\n",
      "        \"Sales Forecast\": \"None\"\n",
      "    },\n",
      "    \"description\": \"This sheet appears to be related to inventory planning as it mentions 'CASES DELIVERY BY MONTH', which suggests tracking of product deliveries over time, a key aspect of managing inventory levels.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_51.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_47.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_53.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_84.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_134.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"The sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Response:\n",
      "{\n",
      "    \"is_inventory_planning\": 0,\n",
      "    \"details\": {\n",
      "        \"SKU\": null,\n",
      "        \"Location/Warehouse\": null,\n",
      "        \"Quantity\": null,\n",
      "        \"Total Inventory\": null,\n",
      "        \"Current Inventory\": null,\n",
      "        \"Sales Forecast\": null\n",
      "    },\n",
      "    \"description\": \"This sheet does not contain any identifiable columns related to inventory planning.\"\n",
      "}\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_90.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_89927/145246337.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  results[file] = response.dict()  # Save structured output\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Define the base folder path for analysis\u001b[39;00m\n\u001b[1;32m    136\u001b[0m base_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/ajay/Documents/Atomic/inventory_analysis_2\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Replace with your folder path\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m analysis_results \u001b[39m=\u001b[39m analyze_csv_files(base_folder, llm)\n\u001b[1;32m    139\u001b[0m \u001b[39m# Print Results\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m file, result \u001b[39min\u001b[39;00m analysis_results\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[98], line 122\u001b[0m, in \u001b[0;36manalyze_csv_files\u001b[0;34m(base_folder, llm)\u001b[0m\n\u001b[1;32m    119\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path)\n\u001b[1;32m    121\u001b[0m \u001b[39m# Analyze the sheet\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m response \u001b[39m=\u001b[39m analyze_sheet(dataframe\u001b[39m=\u001b[39;49mdf, llm\u001b[39m=\u001b[39;49mllm)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    124\u001b[0m     results[file] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()  \u001b[39m# Save structured output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 84\u001b[0m, in \u001b[0;36manalyze_sheet\u001b[0;34m(dataframe, llm, retries)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mwhile\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m         \u001b[39m# Send the prompt to the LLM\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m         response \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mpredict(prompt)\n\u001b[1;32m     86\u001b[0m         \u001b[39m# Clean the response\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         cleaned_response \u001b[39m=\u001b[39m clean_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1054\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m@deprecated\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m0.1.7\u001b[39m\u001b[39m\"\u001b[39m, alternative\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minvoke\u001b[39m\u001b[39m\"\u001b[39m, removal\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1050\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, stop: Optional[Sequence[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1052\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m stop \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m-> 1054\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m([HumanMessage(content\u001b[39m=\u001b[39;49mtext)], stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result\u001b[39m.\u001b[39mcontent, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1056\u001b[0m         \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1017\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39m@deprecated\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m0.1.7\u001b[39m\u001b[39m\"\u001b[39m, alternative\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minvoke\u001b[39m\u001b[39m\"\u001b[39m, removal\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1011\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1016\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m-> 1017\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1018\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1019\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m   1021\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:476\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    471\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    472\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    473\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    474\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    475\u001b[0m }\n\u001b[0;32m--> 476\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    477\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    478\u001b[0m )\n\u001b[1;32m    479\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:387\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    389\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    391\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    860\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    861\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    862\u001b[0m             {\n\u001b[1;32m    863\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    864\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    865\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    866\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    867\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    868\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    869\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    870\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    871\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    872\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    873\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    874\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    875\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    876\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    877\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    878\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    879\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[1;32m    880\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    881\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    882\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    883\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    884\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    885\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    886\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    887\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    888\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    889\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    890\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    891\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    892\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    893\u001b[0m             },\n\u001b[1;32m    894\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    895\u001b[0m         ),\n\u001b[1;32m    896\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    897\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    898\u001b[0m         ),\n\u001b[1;32m    899\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    900\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    902\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    958\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    961\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    962\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mSending HTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl)\n\u001b[1;32m    992\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    994\u001b[0m         request,\n\u001b[1;32m    995\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    996\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    999\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered httpx.TimeoutException\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    927\u001b[0m     request,\n\u001b[1;32m    928\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    929\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    930\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    931\u001b[0m )\n\u001b[1;32m    932\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    955\u001b[0m         request,\n\u001b[1;32m    956\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    957\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    992\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1029\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[39m=\u001b[39m pool_request\u001b[39m.\u001b[39mwait_for_connection(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[39m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    218\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/crustdata/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the structured output schema\n",
    "class InventorySheetAnalysis(BaseModel):\n",
    "    is_inventory_planning: int = Field(\n",
    "        ..., description=\"1 if the sheet is for inventory planning, 0 otherwise\"\n",
    "    )\n",
    "    details: dict = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A dictionary mapping columns to roles: SKU, Location/Warehouse, Quantity, \"\n",
    "            \"Total Inventory, Current Inventory, Sales Forecast. Set to None if not found.\"\n",
    "        ),\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"Explanation of why this sheet is for inventory planning.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to clean the LLM response\n",
    "def clean_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the LLM response by removing Markdown fences and extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        response (str): The raw response from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove Markdown fences (e.g., ```json ... ```)\n",
    "        if response.startswith(\"```json\"):\n",
    "            response = response[7:]  # Remove ```json\n",
    "        if response.endswith(\"```\"):\n",
    "            response = response[:-3]  # Remove ```\n",
    "        return response.strip()  # Strip any extra whitespace\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning response: {e}\")\n",
    "        return response  # Return as-is if cleaning fails\n",
    "\n",
    "\n",
    "# Function to analyze a sheet using GPT-4o-mini\n",
    "def analyze_sheet(dataframe, llm, retries=10):\n",
    "    # Convert the first 5 rows of the dataframe to Markdown\n",
    "    markdown_preview = dataframe.head(5).to_markdown()\n",
    "\n",
    "    # Define the LLM prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a data analysis expert. Below is a preview of a sheet:\n",
    "\n",
    "    {markdown_preview}\n",
    "\n",
    "    Task:\n",
    "    1. Determine if this sheet is used for inventory planning (1 for Yes, 0 for No).\n",
    "    2. If Yes, provide:\n",
    "       - 'details': A dictionary mapping column names to these roles: SKU, Location/Warehouse, Quantity, Total Inventory, Current Inventory, Sales Forecast. If no related column exists, set the value to None.\n",
    "       - 'description': A brief explanation explaining why this sheet is for inventory planning.\n",
    "\n",
    "    Respond strictly in the following JSON format:\n",
    "    {{\n",
    "        \"is_inventory_planning\": 1,\n",
    "        \"details\": {{\n",
    "            \"SKU\": \"Column Name or None\",\n",
    "            \"Location/Warehouse\": \"Column Name or None\",\n",
    "            \"Quantity\": \"Column Name or None\",\n",
    "            \"Total Inventory\": \"Column Name or None\",\n",
    "            \"Current Inventory\": \"Column Name or None\",\n",
    "            \"Sales Forecast\": \"Column Name or None\"\n",
    "        }},\n",
    "        \"description\": \"Brief explanation\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Retry loop\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            # Send the prompt to the LLM\n",
    "            response = llm.predict(prompt)\n",
    "\n",
    "            # Clean the response\n",
    "            cleaned_response = clean_response(response)\n",
    "            print(f\"Cleaned Response:\\n{cleaned_response}\")\n",
    "\n",
    "            # Validate the cleaned response with Pydantic using model_validate_json\n",
    "            structured_output = InventorySheetAnalysis.model_validate_json(cleaned_response)\n",
    "            return structured_output  # Return the validated output if successful\n",
    "\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Validation Error: {ve}\")\n",
    "            retries -= 1\n",
    "            print(f\"Retries left: {retries}\")\n",
    "            if retries == 0:\n",
    "                print(\"Max retries reached. Skipping this sheet.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Function to process all CSV files in a base folder\n",
    "def analyze_csv_files(base_folder, llm):\n",
    "    results = {}\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                    # Load the CSV file\n",
    "                    df = pd.read_csv(file_path)\n",
    "\n",
    "                    # Analyze the sheet\n",
    "                    response = analyze_sheet(dataframe=df, llm=llm)\n",
    "                    if response:\n",
    "                        results[file] = response.dict()  # Save structured output\n",
    "                    else:\n",
    "                        print(f\"Failed to analyze file: {file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "# Initialize the GPT-4o-mini model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define the base folder path for analysis\n",
    "base_folder = \"/Users/ajay/Documents/Atomic/inventory_analysis_2\"  # Replace with your folder path\n",
    "analysis_results = analyze_csv_files(base_folder, llm)\n",
    "\n",
    "# Print Results\n",
    "for file, result in analysis_results.items():\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Inventory Planning: {'Yes' if result['is_inventory_planning'] == 1 else 'No'}\")\n",
    "    if result['is_inventory_planning'] == 1:\n",
    "        print(f\"Details: {result['details']}\")\n",
    "        print(f\"Description: {result['description']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a table contains information related to Inventory - 0.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/True Up Template/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_4.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_5.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_7.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_6.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_8.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_10.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INVENTORY SNAPSHOT/table_9.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_50.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_44.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_78.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_93.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_123.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_137.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_87.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_136.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_86.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_92.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_122.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_79.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_45.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_51.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_47.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_53.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_84.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_134.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_120.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_90.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_108.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_109.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_121.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_91.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_85.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_135.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_52.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_46.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_42.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_56.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_119.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_131.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_81.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_95.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_125.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_94.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_124.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_130.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_80.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_118.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_57.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_43.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_69.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_55.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_41.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_126.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_96.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_82.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_132.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_83.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_133.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_127.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_97.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_40.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_54.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_68.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_2.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 9 column 28 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...aging stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_33.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_27.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_140.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_154.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_155.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_141.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_26.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_32.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_24.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_30.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_18.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_157.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_143.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_142.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_156.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_19.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_31.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_25.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_4.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_21.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_35.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_152.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_146.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_147.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_153.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_34.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_20.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_5.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_7.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 4 column 17 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...future stock needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 4 column 17 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...future stock needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 4 column 17 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...g inventory levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 7\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_36.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_22.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_145.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_151.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_150.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_144.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_23.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_37.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_6.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_12.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_161.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_149.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_148.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_160.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_13.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_8.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_11.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_39.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_162.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_163.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_38.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_10.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_9.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_28.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_14.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_166.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_15.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_29.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_17.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_158.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_164.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_165.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_159.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_16.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_71.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_65.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_59.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_102.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_116.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_117.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_103.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_58.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_64.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_70.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_66.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_72.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_115.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_101.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_129.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_99.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_128.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_98.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_100.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_114.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_73.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_67.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_63.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_77.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_138.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_88.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_110.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_104.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_105.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_111.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_139.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_89.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_76.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_62.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_48.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_74.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_60.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_107.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_113.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_112.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_106.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_61.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_75.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Delivery by Month/table_49.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Sheet24/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Current Inventory/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Current Inventory/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Current Inventory/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Current Inventory/table_4.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Current Inventory/table_5.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 9 column 28 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 9 column 28 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 9 column 28 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 7\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Inventory - Monthly NEW/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INPUT -> DOT Inventory Report/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/INPUT -> DOT Inventory Report/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Item List/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Invoice Tracker(TK)/table_1.csv\n",
      "Results saved to inventory_analysis_results.csv\n",
      "                        sheet_name         file  is_inventory_planning  \\\n",
      "0                 True Up Template  table_2.csv                      0   \n",
      "1                 True Up Template  table_3.csv                      1   \n",
      "2                 True Up Template  table_1.csv                      1   \n",
      "3               INVENTORY SNAPSHOT  table_2.csv                      1   \n",
      "4               INVENTORY SNAPSHOT  table_3.csv                      1   \n",
      "..                             ...          ...                    ...   \n",
      "185        Inventory - Monthly NEW  table_1.csv                      1   \n",
      "186  INPUT -> DOT Inventory Report  table_2.csv                      1   \n",
      "187  INPUT -> DOT Inventory Report  table_1.csv                      1   \n",
      "188                      Item List  table_1.csv                      0   \n",
      "189            Invoice Tracker(TK)  table_1.csv                      1   \n",
      "\n",
      "                                           description                  SKU  \\\n",
      "0    The sheet does not contain identifiable column...                 None   \n",
      "1    This sheet contains data related to product or...          Unnamed: 10   \n",
      "2    This sheet is used for inventory planning as i...           Unnamed: 3   \n",
      "3    This sheet contains information about SKU desc...           Unnamed: 1   \n",
      "4    This sheet appears to be used for inventory pl...                 None   \n",
      "..                                                 ...                  ...   \n",
      "185  This sheet contains data related to production...           Unnamed: 0   \n",
      "186  This sheet contains information about stocked ...           Unnamed: 1   \n",
      "187  This sheet is used for inventory planning as i...      DOT ITEM NUMBER   \n",
      "188  The sheet does not contain any identifiable co...                 None   \n",
      "189  This sheet tracks product shipments, including...  Product Description   \n",
      "\n",
      "    Location/Warehouse     Quantity  Total Inventory Current Inventory  \\\n",
      "0                 None         None             None              None   \n",
      "1             Location        Cases      Unnamed: 12       Unnamed: 13   \n",
      "2                 Date   Unnamed: 4       Unnamed: 6        Unnamed: 5   \n",
      "3           Unnamed: 6   Unnamed: 9       Unnamed: 8       Unnamed: 10   \n",
      "4                 None   Unnamed: 5       Unnamed: 1        Unnamed: 6   \n",
      "..                 ...          ...              ...               ...   \n",
      "185         Unnamed: 1   Unnamed: 2       Unnamed: 3        Unnamed: 4   \n",
      "186         Unnamed: 2   Unnamed: 5       Unnamed: 6        Unnamed: 7   \n",
      "187             DOT DC  QTY ON HAND  OVERALL BALANCE     NET AVAILABLE   \n",
      "188               None         None             None              None   \n",
      "189           Location        Cases             None              None   \n",
      "\n",
      "         Sales Forecast  \n",
      "0                  None  \n",
      "1           Unnamed: 14  \n",
      "2           Unnamed: 12  \n",
      "3           Unnamed: 18  \n",
      "4                  None  \n",
      "..                  ...  \n",
      "185          Unnamed: 5  \n",
      "186          Unnamed: 8  \n",
      "187  CASE WKLY FORECAST  \n",
      "188                None  \n",
      "189                None  \n",
      "\n",
      "[190 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the structured output schema\n",
    "class InventorySheetAnalysis(BaseModel):\n",
    "    is_inventory_planning: int = Field(\n",
    "        ..., description=\"1 if the sheet is for inventory planning, 0 otherwise\"\n",
    "    )\n",
    "    details: dict = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A dictionary mapping columns to roles: SKU, Location/Warehouse, Quantity, \"\n",
    "            \"Total Inventory, Current Inventory, Sales Forecast. Set to None if not found.\"\n",
    "        ),\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"Explanation of why this sheet is for inventory planning.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to clean the LLM response\n",
    "def clean_response(response: str) -> str:\n",
    "    if response.startswith(\"```json\"):\n",
    "        response = response[7:]  # Remove ```json\n",
    "    if response.endswith(\"```\"):\n",
    "        response = response[:-3]  # Remove ```\n",
    "    return response.strip()  # Strip any extra whitespace\n",
    "\n",
    "\n",
    "# Function to analyze a sheet using GPT-4o-mini\n",
    "def analyze_sheet(dataframe, llm, retries=10):\n",
    "    markdown_preview = dataframe.head(5).to_markdown()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data analysis expert. Below is a preview of a sheet:\n",
    "\n",
    "    {markdown_preview}\n",
    "\n",
    "    Task:\n",
    "    1. Determine if this sheet is used for inventory planning (1 for Yes, 0 for No).\n",
    "    2. If Yes, provide:\n",
    "       - 'details': A dictionary mapping column names to these roles: SKU, Location/Warehouse, Quantity, Total Inventory, Current Inventory, Sales Forecast. If no related column exists, set the value to None.\n",
    "       - 'description': A brief explanation explaining why this sheet is for inventory planning.\n",
    "\n",
    "    Respond strictly in the following JSON format:\n",
    "    {{\n",
    "        \"is_inventory_planning\": 1,\n",
    "        \"details\": {{\n",
    "            \"SKU\": \"Column Name or None\",\n",
    "            \"Location/Warehouse\": \"Column Name or None\",\n",
    "            \"Quantity\": \"Column Name or None\",\n",
    "            \"Total Inventory\": \"Column Name or None\",\n",
    "            \"Current Inventory\": \"Column Name or None\",\n",
    "            \"Sales Forecast\": \"Column Name or None\"\n",
    "        }},\n",
    "        \"description\": \"Brief explanation\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = llm.predict(prompt)\n",
    "            cleaned_response = clean_response(response)\n",
    "            structured_output = InventorySheetAnalysis.model_validate_json(cleaned_response)\n",
    "            return structured_output\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Validation Error: {ve}\")\n",
    "            retries -= 1\n",
    "            print(f\"Retries left: {retries}\")\n",
    "            if retries == 0:\n",
    "                print(\"Max retries reached. Skipping this sheet.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Function to process all CSV files in a base folder\n",
    "def analyze_csv_files(base_folder, llm):\n",
    "    results = []\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    response = analyze_sheet(dataframe=df, llm=llm)\n",
    "                    if response:\n",
    "                        # Unpack the details dictionary into individual columns\n",
    "                        details = response.details\n",
    "                        results.append(\n",
    "                            {\n",
    "                                'sheet_name': folder,\n",
    "                                \"file\": file,\n",
    "                                \"is_inventory_planning\": response.is_inventory_planning,\n",
    "                                \"description\": response.description,\n",
    "                                \"SKU\": details.get(\"SKU\"),\n",
    "                                \"Location/Warehouse\": details.get(\"Location/Warehouse\"),\n",
    "                                \"Quantity\": details.get(\"Quantity\"),\n",
    "                                \"Total Inventory\": details.get(\"Total Inventory\"),\n",
    "                                \"Current Inventory\": details.get(\"Current Inventory\"),\n",
    "                                \"Sales Forecast\": details.get(\"Sales Forecast\"),\n",
    "                            }\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"Failed to analyze file: {file}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "base_folder = \"/Users/ajay/Documents/Atomic/inventory_analysis_2\"\n",
    "analysis_results_df = analyze_csv_files(base_folder, llm)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_file = \"inventory_analysis_results.csv\"\n",
    "analysis_results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Print the DataFrame\n",
    "print(analysis_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sheet_name</th>\n",
       "      <th>file</th>\n",
       "      <th>is_inventory_planning</th>\n",
       "      <th>description</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Location/Warehouse</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Total Inventory</th>\n",
       "      <th>Current Inventory</th>\n",
       "      <th>Sales Forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True Up Template</td>\n",
       "      <td>table_2.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>The sheet does not contain identifiable column...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True Up Template</td>\n",
       "      <td>table_3.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet contains data related to product or...</td>\n",
       "      <td>Unnamed: 10</td>\n",
       "      <td>Location</td>\n",
       "      <td>Cases</td>\n",
       "      <td>Unnamed: 12</td>\n",
       "      <td>Unnamed: 13</td>\n",
       "      <td>Unnamed: 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True Up Template</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet is used for inventory planning as i...</td>\n",
       "      <td>Unnamed: 3</td>\n",
       "      <td>Date</td>\n",
       "      <td>Unnamed: 4</td>\n",
       "      <td>Unnamed: 6</td>\n",
       "      <td>Unnamed: 5</td>\n",
       "      <td>Unnamed: 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INVENTORY SNAPSHOT</td>\n",
       "      <td>table_2.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet contains information about SKU desc...</td>\n",
       "      <td>Unnamed: 1</td>\n",
       "      <td>Unnamed: 6</td>\n",
       "      <td>Unnamed: 9</td>\n",
       "      <td>Unnamed: 8</td>\n",
       "      <td>Unnamed: 10</td>\n",
       "      <td>Unnamed: 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INVENTORY SNAPSHOT</td>\n",
       "      <td>table_3.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet appears to be used for inventory pl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Unnamed: 5</td>\n",
       "      <td>Unnamed: 1</td>\n",
       "      <td>Unnamed: 6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Inventory - Monthly NEW</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet contains data related to production...</td>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Unnamed: 1</td>\n",
       "      <td>Unnamed: 2</td>\n",
       "      <td>Unnamed: 3</td>\n",
       "      <td>Unnamed: 4</td>\n",
       "      <td>Unnamed: 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>INPUT -&gt; DOT Inventory Report</td>\n",
       "      <td>table_2.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet contains information about stocked ...</td>\n",
       "      <td>Unnamed: 1</td>\n",
       "      <td>Unnamed: 2</td>\n",
       "      <td>Unnamed: 5</td>\n",
       "      <td>Unnamed: 6</td>\n",
       "      <td>Unnamed: 7</td>\n",
       "      <td>Unnamed: 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>INPUT -&gt; DOT Inventory Report</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet is used for inventory planning as i...</td>\n",
       "      <td>DOT ITEM NUMBER</td>\n",
       "      <td>DOT DC</td>\n",
       "      <td>QTY ON HAND</td>\n",
       "      <td>OVERALL BALANCE</td>\n",
       "      <td>NET AVAILABLE</td>\n",
       "      <td>CASE WKLY FORECAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Item List</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>The sheet does not contain any identifiable co...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Invoice Tracker(TK)</td>\n",
       "      <td>table_1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>This sheet tracks product shipments, including...</td>\n",
       "      <td>Product Description</td>\n",
       "      <td>Location</td>\n",
       "      <td>Cases</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        sheet_name         file  is_inventory_planning  \\\n",
       "0                 True Up Template  table_2.csv                      0   \n",
       "1                 True Up Template  table_3.csv                      1   \n",
       "2                 True Up Template  table_1.csv                      1   \n",
       "3               INVENTORY SNAPSHOT  table_2.csv                      1   \n",
       "4               INVENTORY SNAPSHOT  table_3.csv                      1   \n",
       "..                             ...          ...                    ...   \n",
       "185        Inventory - Monthly NEW  table_1.csv                      1   \n",
       "186  INPUT -> DOT Inventory Report  table_2.csv                      1   \n",
       "187  INPUT -> DOT Inventory Report  table_1.csv                      1   \n",
       "188                      Item List  table_1.csv                      0   \n",
       "189            Invoice Tracker(TK)  table_1.csv                      1   \n",
       "\n",
       "                                           description                  SKU  \\\n",
       "0    The sheet does not contain identifiable column...                 None   \n",
       "1    This sheet contains data related to product or...          Unnamed: 10   \n",
       "2    This sheet is used for inventory planning as i...           Unnamed: 3   \n",
       "3    This sheet contains information about SKU desc...           Unnamed: 1   \n",
       "4    This sheet appears to be used for inventory pl...                 None   \n",
       "..                                                 ...                  ...   \n",
       "185  This sheet contains data related to production...           Unnamed: 0   \n",
       "186  This sheet contains information about stocked ...           Unnamed: 1   \n",
       "187  This sheet is used for inventory planning as i...      DOT ITEM NUMBER   \n",
       "188  The sheet does not contain any identifiable co...                 None   \n",
       "189  This sheet tracks product shipments, including...  Product Description   \n",
       "\n",
       "    Location/Warehouse     Quantity  Total Inventory Current Inventory  \\\n",
       "0                 None         None             None              None   \n",
       "1             Location        Cases      Unnamed: 12       Unnamed: 13   \n",
       "2                 Date   Unnamed: 4       Unnamed: 6        Unnamed: 5   \n",
       "3           Unnamed: 6   Unnamed: 9       Unnamed: 8       Unnamed: 10   \n",
       "4                 None   Unnamed: 5       Unnamed: 1        Unnamed: 6   \n",
       "..                 ...          ...              ...               ...   \n",
       "185         Unnamed: 1   Unnamed: 2       Unnamed: 3        Unnamed: 4   \n",
       "186         Unnamed: 2   Unnamed: 5       Unnamed: 6        Unnamed: 7   \n",
       "187             DOT DC  QTY ON HAND  OVERALL BALANCE     NET AVAILABLE   \n",
       "188               None         None             None              None   \n",
       "189           Location        Cases             None              None   \n",
       "\n",
       "         Sales Forecast  \n",
       "0                  None  \n",
       "1           Unnamed: 14  \n",
       "2           Unnamed: 12  \n",
       "3           Unnamed: 18  \n",
       "4                  None  \n",
       "..                  ...  \n",
       "185          Unnamed: 5  \n",
       "186          Unnamed: 8  \n",
       "187  CASE WKLY FORECAST  \n",
       "188                None  \n",
       "189                None  \n",
       "\n",
       "[190 rows x 10 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Table Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost  for sheet 1 : $1.44 (ran with gpt-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      " INVENTORY SNAPSHOT\n",
      "Starting analysis for sheet: INVENTORY SNAPSHOT\n",
      "Fetching 50xN chunk from sheet: INVENTORY SNAPSHOT\n",
      "Fetched chunk from sheet 'INVENTORY SNAPSHOT':\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, 2025-01-10 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 45 = antsy, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, N...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INVENTORY SNAPSHOT')\n",
      "\n",
      "# Create a new folder named 'INVENTORY SNAPSHOT'\n",
      "if not os.path.exists('INVENTORY SNAPSHOT'):\n",
      "    os.makedirs('INVENTORY SNAPSHOT')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate o...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INVENTORY SNAPSHOT')\n",
      "\n",
      "# Create a new folder named 'INVENTORY SNAPSHOT'\n",
      "if not os.path.exists('INVENTORY SNAPSHOT'):\n",
      "    os.makedirs('INVENTORY SNAPSHOT')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is e...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INVENTORY SNAPSHOT')\n",
      "\n",
      "# Create a new folder named 'INVENTORY SNAPSHOT'\n",
      "if not os.path.exists('INVENTORY SNAPSHOT'):\n",
      "    os.makedirs('INVENTORY SNAPSHOT')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty (all values are NaN), it's the end of a table\n",
      "    if df.iloc[i].isna().all():\n",
      "        # Extract the table\n",
      "        table = df.iloc[start_row:i]\n",
      "        # Save the table as a separate CSV file in the folder\n",
      "        table.to_csv(f'INVENTORY SNAPSHOT/table_{table_number}.csv', index=False)\n",
      "        # Update the start_row to the next row\n",
      "        start_row = i + 1\n",
      "        # Increment the table_number\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table (if any)\n",
      "if start_row < len(df):\n",
      "    table = df.iloc[start_row:]\n",
      "    table.to_csv(f'INVENTORY SNAPSHOT/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Sheet24\n",
      "Starting analysis for sheet: Sheet24\n",
      "Fetching 50xN chunk from sheet: Sheet24\n",
      "Fetched chunk from sheet 'Sheet24':\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "None, SKU Description, Item Code, Comarco, Lineage, Dot, Producing Feb 3\n",
      "None, CP Big Mozz Sticks, 20005.0, None, 17862.0, 7612.0, None, None, None, None\n",
      "None, CP Mozz Nuggets, 10150.0, 5585.0, 7149.0, 10744.0, None, None, None, None\n",
      "None, CP The Mozz Junior, 40005.0, None, 2462.0, 1024.0, 1560.0, None, None, None\n",
      "None, CP 3.75\" Big Mozz Sticks, 37505.0, None, 342.0, 0.0, 3120.0, None, None, None\n",
      "None, CP FINALMENTE Mozz Junior, 40055.0, None, 1873.0, 0.0, None, None, None, None\n",
      "None, CP Ret...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that fulfills the tasks:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Sheet24')\n",
      "\n",
      "# Create a new folder named 'Sheet24'\n",
      "if not os.path.exists('Sheet24'):\n",
      "    os.makedirs('Sheet24')\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull...\n",
      "Extracted code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Sheet24')\n",
      "\n",
      "# Create a new folder named 'Sheet24'\n",
      "if not os.path.exists('Sheet24'):\n",
      "    os.makedirs('Sheet24')\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # Check if the row is blank\n",
      "        if current_ta...\n",
      "Executing the following code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Sheet24')\n",
      "\n",
      "# Create a new folder named 'Sheet24'\n",
      "if not os.path.exists('Sheet24'):\n",
      "    os.makedirs('Sheet24')\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # Check if the row is blank\n",
      "        if current_table:  # If the current table is not empty, add it to the tables list\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        current_table.append(df.iloc[i])\n",
      "\n",
      "# If there is a table at the end of the file, add it to the tables list\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Extract each identified table and save it as a separate CSV file in the folder\n",
      "for i, table in enumerate(tables):\n",
      "    table.to_csv(f'Sheet24/table_{i+1}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Current Inventory\n",
      "Starting analysis for sheet: Current Inventory\n",
      "Fetching 50xN chunk from sheet: Current Inventory\n",
      "Fetched chunk from sheet 'Current Inventory':\n",
      "INVENTORY BY LOCATION, None, None, None, None, INVENTORY TOTALS -- EXCLUDING EGS, None, None, None\n",
      "None, None, None, None, None, None, None, None, None\n",
      "Date, 2025-01-05 00:00:00, None, None, None, Date, 2024-11-14 00:00:00, None, None\n",
      "None, None, None, None, None, None, None, None\n",
      "Location, Description, Code, Case Quantity per Tracker AS OF DATE, None, Description, Code, Case Quantity per Tracker AS OF DATE\n",
      "Comarco Products, CP Big Mozz Sticks, 20005-1, =sumifs('Inventory Detail'!$K:$K,'Inventor...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory')\n",
      "\n",
      "# Create a new folder named 'Current Inventory'\n",
      "if not os.path.exists('Current Inventory'):\n",
      "    os.makedirs('Current Inventory')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory')\n",
      "\n",
      "# Create a new folder named 'Current Inventory'\n",
      "if not os.path.exists('Current Inventory'):\n",
      "    os.makedirs('Current Inventory')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty (all values are NaN), it's t...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Current Inventory')\n",
      "\n",
      "# Create a new folder named 'Current Inventory'\n",
      "if not os.path.exists('Current Inventory'):\n",
      "    os.makedirs('Current Inventory')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty (all values are NaN), it's the end of a table\n",
      "    if df.iloc[i].isna().all():\n",
      "        # Extract the table\n",
      "        table = df.iloc[start_row:i]\n",
      "        # Save the table as a CSV file\n",
      "        table.to_csv(f'Current Inventory/table_{table_number}.csv', index=False)\n",
      "        # Update the start_row to the next row\n",
      "        start_row = i + 1\n",
      "        # Increment the table_number\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table (if any)\n",
      "if start_row < len(df):\n",
      "    table = df.iloc[start_row:]\n",
      "    table.to_csv(f'Current Inventory/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " INPUT -> DOT Inventory Report\n",
      "Starting analysis for sheet: INPUT -> DOT Inventory Report\n",
      "Fetching 50xN chunk from sheet: INPUT -> DOT Inventory Report\n",
      "Fetched chunk from sheet 'INPUT -> DOT Inventory Report':\n",
      "SUPPLIER STOCKED ITEMS STATUS REPORT, None, None, 2024-10-21 00:00:00\n",
      "DATA DATE, DOT ITEM NUMBER, ITEM DESCRIPTION, TEMP, STOCK STATUS, PROD SHELF LIFE, MFG NUMBER, UPC NUMBER, LINE ID, LINE ID NAME, DOT DC, QTY ON HAND, ON ORDER, INTERNAL TRANSFERS, SOLD, TRANSFER QTY CMTD, NET AVAILABLE, OVERALL BALANCE, NOT AVAILABLE INVENTORY, DAYS INV IN PIPELINE, CASE WKLY FORECAST, SEASONAL FORECAST, LAST WEEKLY FORECAST, SEASONAL PROFILE #, SHIPPED PRD TO DATE, SHIPPED LAST PRD, SHIPPED 2 PRD AGO, SHIPPE...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INPUT -> DOT Inventory Report')\n",
      "\n",
      "# Create a new folder\n",
      "os.makedirs('INPUT -> DOT Inventory Report', exist_ok=True)\n",
      "\n",
      "# Identify tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    # If the row is not empty, add it to the current table\n",
      "    if df.i...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INPUT -> DOT Inventory Report')\n",
      "\n",
      "# Create a new folder\n",
      "os.makedirs('INPUT -> DOT Inventory Report', exist_ok=True)\n",
      "\n",
      "# Identify tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    # If the row is not empty, add it to the current table\n",
      "    if df.iloc[i].notna().any():\n",
      "        current_table.append(df.iloc[i])\n",
      " ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='INPUT -> DOT Inventory Report')\n",
      "\n",
      "# Create a new folder\n",
      "os.makedirs('INPUT -> DOT Inventory Report', exist_ok=True)\n",
      "\n",
      "# Identify tables\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    # If the row is not empty, add it to the current table\n",
      "    if df.iloc[i].notna().any():\n",
      "        current_table.append(df.iloc[i])\n",
      "    # If the row is empty and there is a current table, save the current table and start a new one\n",
      "    elif len(current_table) > 0:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "        current_table = []\n",
      "\n",
      "# Save the last table if it exists\n",
      "if len(current_table) > 0:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    table.to_csv(f'INPUT -> DOT Inventory Report/table_{i+1}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Inventory Detail\n",
      "Starting analysis for sheet: Inventory Detail\n",
      "Fetching 50xN chunk from sheet: Inventory Detail\n",
      "Fetched chunk from sheet 'Inventory Detail':\n",
      "Invoice Sent, Order Date, PickUp Date, ORIGIN, Customer Delivery, DESTINATION, Transport, BOL #, PO #, Product Description, Cases , Pallets, FTL, Product UPC, Product Code, Month, Invoice Date, PO #, Invoice Number, Total Invoice, Invoice $ per Case, Notes, Week, Year, None, None, None, None, None, None\n",
      "True Up, None, 2023-07-18 00:00:00, EGS- Kansas City, KS, True Up, None, None, None, None, Big Mozz Sticks, 531.0, =ifna(K2/vlookup(J2,'Item List'!A:E,5,false),\"\"), =ifna(L2/vlookup(J2,'Item List...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9693 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9693 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      " True Up Template\n",
      "Starting analysis for sheet: True Up Template\n",
      "Fetching 50xN chunk from sheet: True Up Template\n",
      "Fetched chunk from sheet 'True Up Template':\n",
      "Date, 2024-10-03 00:00:00, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, COUNTS FROM WAREHOUSES, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "Location, Description, Code, Case Quantity per Tracker AS OF DATE, Actual Quantity AS OF DATE, Difference, None, None, None, None, None, Location, Description, Jan 11th 2024, Jan 21st 2024, 2024-02-01 00:00:00, Mid-Fe...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='True Up Template')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('True Up Template'):\n",
      "    os.makedirs('True Up Template')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='True Up Template')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('True Up Template'):\n",
      "    os.makedirs('True Up Template')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty (all values are NaN), it's the end of a table\n",
      "    if df.iloc[i]....\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='True Up Template')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('True Up Template'):\n",
      "    os.makedirs('True Up Template')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is empty (all values are NaN), it's the end of a table\n",
      "    if df.iloc[i].isna().all():\n",
      "        # Extract the table\n",
      "        table = df.iloc[start_row:i]\n",
      "        # Save the table to a CSV file\n",
      "        table.to_csv(f'True Up Template/table_{table_number}.csv', index=False)\n",
      "        # Update the start_row to the next row\n",
      "        start_row = i + 1\n",
      "        # Increment the table_number\n",
      "        table_number += 1\n",
      "\n",
      "# Save the last table\n",
      "table = df.iloc[start_row:]\n",
      "table.to_csv(f'True Up Template/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Copy of Inventory Detail\n",
      "Starting analysis for sheet: Copy of Inventory Detail\n",
      "Fetching 50xN chunk from sheet: Copy of Inventory Detail\n",
      "Fetched chunk from sheet 'Copy of Inventory Detail':\n",
      "Order Received, Order Date, PickUp Date, Location, Production, Destination, Transport, BOL #, PO #, Product Description, Cases , Pallets, FTL, Product UPC, Product Code, Month, Invoice Date, PO #, Invoice Number, Total Invoice, Invoice $ per Case, Notes, None, None, None, None, None, None, None, None\n",
      "True Up, None, 2023-07-18 00:00:00, EGS- Kansas City, KS, True Up, None, None, None, None, Big Mozz Sticks, 531.0, =ifna(K2/vlookup(J2,'Item List'!A:D,4,false),\"\"), =ifna(L2/vlookup(J2,'Item List'!A...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9206 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9206 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      " Production by Month\n",
      "Starting analysis for sheet: Production by Month\n",
      "Fetching 50xN chunk from sheet: Production by Month\n",
      "Fetched chunk from sheet 'Production by Month':\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None\n",
      "None, None, Non...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Based on the provided data, it appears that there are no tables present in the chunk. All rows are either blank or contain no data (None). Therefore, there are no tables to extract and save as separate CSV files.\n",
      "\n",
      "However, if there were tables in the data, the Python code to perform the tasks would look something like this:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company...\n",
      "Extracted code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Production by Month')\n",
      "\n",
      "# Create a new folder named 'Production by Month'\n",
      "os.makedirs('Production by Month', exist_ok=True)\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is blank or contains no data\n",
      "    if df.iloc[...\n",
      "Executing the following code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Production by Month')\n",
      "\n",
      "# Create a new folder named 'Production by Month'\n",
      "os.makedirs('Production by Month', exist_ok=True)\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row is blank or contains no data\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # If the start row is not the current row (i.e., a table has been found)\n",
      "        if start_row != i:\n",
      "            # Extract the table and save it as a CSV file\n",
      "            table = df.iloc[start_row:i]\n",
      "            table.to_csv(f'Production by Month/table_{table_number}.csv', index=False)\n",
      "            table_number += 1\n",
      "        # Update the start row\n",
      "        start_row = i + 1\n",
      "\n",
      "# If the last row of the DataFrame is not blank or contains data\n",
      "if start_row != len(df):\n",
      "    # Extract the table and save it as a CSV file\n",
      "    table = df.iloc[start_row:]\n",
      "    table.to_csv(f'Production by Month/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Delivery by Month\n",
      "Starting analysis for sheet: Delivery by Month\n",
      "Fetching 50xN chunk from sheet: Delivery by Month\n",
      "Fetched chunk from sheet 'Delivery by Month':\n",
      "None, None\n",
      "\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, FOODSERVICE, RETAIL, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, =sum(L4,J4,B4,C4,D4,E4,F4,I4,N4), =sum(K4,H4,G4), None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, =sum(L5,J5,B5,C5,D5,E5,F5,I5,N5), =sum(K5,H5,G5), None, None, Non...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is empty\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Save the ta...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is empty\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Save the table to a csv file\n",
      "        df.iloc[start_row:i].to_csv(f'Delivery by Month/table_{table_number}.csv', index=False)\n",
      "        table_number += 1\n",
      "        start_row = i + 1\n",
      "\n",
      "# Save the last table\n",
      "df.iloc[start_row:].to_csv(f'Delivery by Month/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Inventory - Monthly NEW\n",
      "Starting analysis for sheet: Inventory - Monthly NEW\n",
      "Fetching 50xN chunk from sheet: Inventory - Monthly NEW\n",
      "Fetched chunk from sheet 'Inventory - Monthly NEW':\n",
      "None, None, None, None, None, None, None, None\n",
      "DOES NOT INCLUDE PRODUCTION OR DELIVERY FROM RITE STUFF, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "Description, None, None, 2024-01-01 00:00:00, 2024-02-01 00:00:00, 2024-03-01 00:00:00, 2024-04-01 00:00:00, 2024-04-30 00:00:00\n",
      "CP Big Mozz Sticks, =vlookup(A4,'Item List'!A:C,2,false), Starting, =sum(E4:F4,D5:E6), 1559.0, 1.0, None, 1224.0, None...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "os.makedirs('Inventory - Monthly NEW', exist_ok=True)\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "os.makedirs('Inventory - Monthly NEW', exist_ok=True)\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # Check if the row is blank\n",
      "        if curre...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "os.makedirs('Inventory - Monthly NEW', exist_ok=True)\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # Check if the row is blank\n",
      "        if current_table:  # If the current table is not empty, append it to the tables list\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        current_table.append(df.iloc[i].values.tolist())\n",
      "if current_table:  # Append the last table if it's not empty\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Extract each identified table and save it as a separate CSV file in the folder\n",
      "for i, table in enumerate(tables):\n",
      "    table.to_csv(f'Inventory - Monthly NEW/table_{i+1}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Item List\n",
      "Starting analysis for sheet: Item List\n",
      "Fetching 50xN chunk from sheet: Item List\n",
      "Fetched chunk from sheet 'Item List':\n",
      "=IFERROR(__xludf.DUMMYFUNCTION(\"IMPORTRANGE(\"\"https://docs.google.com/spreadsheets/d/11o2iEhkjKpHigHzvgQWiSZOP0As_tE5dvsHELGUZugk\"\",\"\"Items!A:U\"\")\"),\"#REF!\"), None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, N...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Based on the provided data, it appears that there is no actual data present in the chunk. All the rows contain 'None' values. Therefore, there are no tables to extract. However, if we assume that this is a placeholder for actual data, I can provide a Python code that reads data from an Excel file and saves each table as a separate CSV file.\n",
      "\n",
      "Here is the Python code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventor...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Item List')\n",
      "\n",
      "# Create a new folder named 'Item List'\n",
      "if not os.path.exists('Item List'):\n",
      "    os.makedirs('Item List')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row contains all 'None' values, it is considered as the end of a table\n",
      "  ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Item List')\n",
      "\n",
      "# Create a new folder named 'Item List'\n",
      "if not os.path.exists('Item List'):\n",
      "    os.makedirs('Item List')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # If the row contains all 'None' values, it is considered as the end of a table\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Extract the table\n",
      "        table = df.iloc[start_row:i]\n",
      "        \n",
      "        # Save the table as a CSV file\n",
      "        table.to_csv(f'Item List/table_{table_number}.csv', index=False)\n",
      "        \n",
      "        # Update the start row and table number\n",
      "        start_row = i + 1\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table\n",
      "table = df.iloc[start_row:]\n",
      "table.to_csv(f'Item List/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Invoice Tracker(TK)\n",
      "Starting analysis for sheet: Invoice Tracker(TK)\n",
      "Fetching 50xN chunk from sheet: Invoice Tracker(TK)\n",
      "Fetched chunk from sheet 'Invoice Tracker(TK)':\n",
      "Ship Date, PO #, Cases , Product Description, Invoice #, Invoice Date, Location, Type, Description, BOL #, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "=#REF!, =#REF!, =#REF!, =#REF!, None, None, =#REF!, =#REF!, =#REF!, =#REF!, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "='Inventory Detail'!C185, ='Inventory Detail'!I185, ='Inventory Detail'!K185, ='Inventory Detail'!J185,...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Itera...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # Check if th...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is blank or contains no data\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Extract the table and save it as a separate CSV file in the folder\n",
      "        table = df.iloc[start_row:i]\n",
      "        table.to_csv(f'Invoice Tracker(TK)/table_{table_number}.csv', index=False)\n",
      "        # Update the start row and table number\n",
      "        start_row = i + 1\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table if it exists\n",
      "if start_row < len(df):\n",
      "    table = df.iloc[start_row:]\n",
      "    table.to_csv(f'Invoice Tracker(TK)/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx'\n",
    "workbook = load_workbook(file_path, data_only=False, read_only=True)\n",
    "sheet_names = [sheet_name for sheet_name in workbook.sheetnames if not workbook[sheet_name].sheet_state == 'hidden']\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name not in sheet_names:\n",
    "        log_message = f\"Sheet '{sheet_name}' doesn't exist. Available sheets: {sheet_names}\"\n",
    "        print(log_message)\n",
    "        return log_message\n",
    "    print(f\"Fetching 50xN chunk from sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    print(f\"Fetched chunk from sheet '{sheet_name}':\\n{formatted_chunk[:500]}...\")  # Show first 500 chars for brevity\n",
    "    return formatted_chunk\n",
    "\n",
    "# Function to analyze a chunk and generate code\n",
    "import re\n",
    "\n",
    "# def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "#     llm_input = f\"\"\"\n",
    "#     You are a Python code generator for table extraction.\n",
    "#     Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "#     {chunk}\n",
    "\n",
    "#     Tasks:\n",
    "#     1. Analyze the data to identify all tables present.\n",
    "#     2. Generate Python code that reads data from the provided Excel file ('{file_path}') and the specified sheet ('{sheet_name}').\n",
    "#     3. Create a new folder named '{sheet_name}' in the current working directory.\n",
    "#     4. Extract each identified table and save it as a separate CSV file in the folder '{sheet_name}'.\n",
    "#     \"\"\"\n",
    "#     print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "#     llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "#     try:\n",
    "#         response = llm.invoke(llm_input)\n",
    "#         generated_response = response.content  # Extract string content\n",
    "#         print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "#         # Extract Python code block using regex\n",
    "#         match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "#         if match:\n",
    "#             generated_code = match.group(1).strip()  # Extract the Python code\n",
    "#             print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "#             return generated_code\n",
    "#         else:\n",
    "#             print(\"No valid Python code block found in the LLM response.\")\n",
    "#             return \"Error: No valid Python code block found.\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during code generation: {e}\")\n",
    "#         return f\"Error during code generation: {e}\"\n",
    "\n",
    "# best working - Only gets all the csvs. Doesn't get formulas yet\n",
    "def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present in the chunk. Tables are defined as contiguous blocks of data separated by blank rows or rows with no data.\n",
    "    2. Generate Python code that:\n",
    "       - Reads data from the provided Excel file ('{file_path}') and specified sheet ('{sheet_name}').\n",
    "       - Creates a new folder named '{sheet_name}'.\n",
    "       - Extracts each identified table based on the above definition and saves it as a separate CSV file in the folder.\n",
    "    3. Do not make any assumptions about the meaning or functionality of specific columns unless explicitly mentioned.\n",
    "    \"\"\"\n",
    "    print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    try:\n",
    "        response = llm.invoke(llm_input)\n",
    "        generated_response = response.content  # Extract string content\n",
    "        print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "        # Extract Python code block using regex\n",
    "        match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "        if match:\n",
    "            generated_code = match.group(1).strip()  # Extract the Python code\n",
    "            print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "            return generated_code\n",
    "        else:\n",
    "            print(\"No valid Python code block found in the LLM response.\")\n",
    "            return \"Error: No valid Python code block found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code generation: {e}\")\n",
    "        return f\"Error during code generation: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to execute generated Python code\n",
    "def execute_code(code: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executing the following code:\\n{code}...\")  # Log the extracted code\n",
    "        repl_tool = PythonREPLTool()\n",
    "        result = repl_tool.run(code)\n",
    "        print(\"Execution result:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"Execution Error: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return error_message\n",
    "\n",
    "# Main execution process\n",
    "def run_analysis(sheet_name):\n",
    "    print(f\"Starting analysis for sheet: {sheet_name}\")\n",
    "    \n",
    "    # Step 1: Fetch sheet chunk\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    if \"doesn't exist\" in chunk:\n",
    "        print(f\"Error: {chunk}\")\n",
    "        return chunk\n",
    "\n",
    "    # Step 2: Analyze and generate code\n",
    "    print(\"Analyzing the data and generating Python code...\")\n",
    "    code = analyze_and_generate_code(sheet_name, chunk)\n",
    "    if \"Error during code generation\" in code:\n",
    "        return code\n",
    "\n",
    "    # Step 3: Attempt to execute code with retries\n",
    "    retry = 10\n",
    "    while retry > 0:\n",
    "        result = execute_code(code)\n",
    "        if \"Execution Error\" not in result:\n",
    "            print(\"Code executed successfully!\")\n",
    "            return result  # Successful execution\n",
    "\n",
    "        # Provide feedback and request fixes\n",
    "        error_message = f\"\\n\\nError Message: {result}\"\n",
    "        print(f\"Retrying... ({10 - retry + 1}/10)\")\n",
    "        print(f\"Requesting LLM to fix the code. Error: {error_message}\")\n",
    "        code = analyze_and_generate_code(sheet_name, chunk + error_message)\n",
    "        retry -= 1\n",
    "\n",
    "    print(\"Exceeded maximum retries. Could not execute code successfully.\")\n",
    "    return \"Failed after multiple retries.\"\n",
    "\n",
    "# Run the workflow\n",
    "# sheet_to_analyze = \"True Up Template\"  # Replace with your target sheet name\n",
    "\n",
    "for sheet_to_analyze in sheet_names:\n",
    "    print('-'*40,f'\\n {sheet_to_analyze}')\n",
    "    print(run_analysis(sheet_to_analyze))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      " Delivery by Month\n",
      "Starting analysis for sheet: Delivery by Month\n",
      "Fetching 50xN chunk from sheet: Delivery by Month\n",
      "Fetched chunk from sheet 'Delivery by Month':\n",
      "None, None\n",
      "\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, FOODSERVICE, RETAIL, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, =sum(L4,J4,B4,C4,D4,E4,F4,I4,N4), =sum(K4,H4,G4), None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, =sum(L5,J5,B5,C5,D5,E5,F5,I5,N5), =sum(K5,H5,G5), None, None, Non...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is empty\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Save the pr...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read the excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Delivery by Month')\n",
      "\n",
      "# Create a new directory\n",
      "if not os.path.exists('Delivery by Month'):\n",
      "    os.makedirs('Delivery by Month')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the dataframe\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is empty\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Save the previous rows as a table\n",
      "        table = df.iloc[start_row:i]\n",
      "        table.to_csv(f'Delivery by Month/table_{table_number}.csv', index=False)\n",
      "        table_number += 1\n",
      "        # Update the start row\n",
      "        start_row = i + 1\n",
      "\n",
      "# Save the last table\n",
      "table = df.iloc[start_row:]\n",
      "table.to_csv(f'Delivery by Month/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Inventory - Monthly NEW\n",
      "Starting analysis for sheet: Inventory - Monthly NEW\n",
      "Fetching 50xN chunk from sheet: Inventory - Monthly NEW\n",
      "Fetched chunk from sheet 'Inventory - Monthly NEW':\n",
      "None, None, None, None, None, None, None, None\n",
      "DOES NOT INCLUDE PRODUCTION OR DELIVERY FROM RITE STUFF, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "Description, None, None, 2024-01-01 00:00:00, 2024-02-01 00:00:00, 2024-03-01 00:00:00, 2024-04-01 00:00:00, 2024-04-30 00:00:00\n",
      "CP Big Mozz Sticks, =vlookup(A4,'Item List'!A:C,2,false), Starting, =sum(E4:F4,D5:E6), 1559.0, 1.0, None, 1224.0, None...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that fulfills the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "if not os.path.exists('Inventory - Monthly NEW'):\n",
      "    os.makedirs('Inventory - Monthly NEW')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "if not os.path.exists('Inventory - Monthly NEW'):\n",
      "    os.makedirs('Inventory - Monthly NEW')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      " ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Inventory - Monthly NEW')\n",
      "\n",
      "# Create a new folder named 'Inventory - Monthly NEW'\n",
      "if not os.path.exists('Inventory - Monthly NEW'):\n",
      "    os.makedirs('Inventory - Monthly NEW')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is blank\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Extract the table\n",
      "        table = df.iloc[start_row:i]\n",
      "        # Save the table as a separate CSV file in the folder\n",
      "        table.to_csv(f'Inventory - Monthly NEW/table_{table_number}.csv', index=False)\n",
      "        # Update the start row and table number\n",
      "        start_row = i + 1\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table\n",
      "table = df.iloc[start_row:]\n",
      "# Save the last table as a separate CSV file in the folder\n",
      "table.to_csv(f'Inventory - Monthly NEW/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Item List\n",
      "Starting analysis for sheet: Item List\n",
      "Fetching 50xN chunk from sheet: Item List\n",
      "Fetched chunk from sheet 'Item List':\n",
      "=IFERROR(__xludf.DUMMYFUNCTION(\"IMPORTRANGE(\"\"https://docs.google.com/spreadsheets/d/11o2iEhkjKpHigHzvgQWiSZOP0As_tE5dvsHELGUZugk\"\",\"\"Items!A:U\"\")\"),\"#REF!\"), None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None\n",
      "None, None, None, N...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Based on the provided data, it appears that there is no actual data present in the chunk. All the rows contain 'None' values. Therefore, there are no tables to extract. However, if there were tables, the Python code to perform the tasks would look something like this:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Item List')\n",
      "\n",
      "# Create a new folder...\n",
      "Extracted code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Item List')\n",
      "\n",
      "# Create a new folder named 'Item List'\n",
      "if not os.path.exists('Item List'):\n",
      "    os.makedirs('Item List')\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # If the row is blank\n",
      "        if current_table:  # If there i...\n",
      "Executing the following code:\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Read data from the Excel file\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Item List')\n",
      "\n",
      "# Create a new folder named 'Item List'\n",
      "if not os.path.exists('Item List'):\n",
      "    os.makedirs('Item List')\n",
      "\n",
      "# Identify all tables present in the chunk\n",
      "tables = []\n",
      "current_table = []\n",
      "for i in range(len(df)):\n",
      "    if df.iloc[i].isnull().all():  # If the row is blank\n",
      "        if current_table:  # If there is a table being built\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        current_table.append(df.iloc[i].values.tolist())\n",
      "if current_table:  # Add the last table if it exists\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    table.to_csv(f'Item List/table_{i+1}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n",
      "---------------------------------------- \n",
      " Invoice Tracker(TK)\n",
      "Starting analysis for sheet: Invoice Tracker(TK)\n",
      "Fetching 50xN chunk from sheet: Invoice Tracker(TK)\n",
      "Fetched chunk from sheet 'Invoice Tracker(TK)':\n",
      "Ship Date, PO #, Cases , Product Description, Invoice #, Invoice Date, Location, Type, Description, BOL #, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "=#REF!, =#REF!, =#REF!, =#REF!, None, None, =#REF!, =#REF!, =#REF!, =#REF!, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "='Inventory Detail'!C185, ='Inventory Detail'!I185, ='Inventory Detail'!K185, ='Inventory Detail'!J185,...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "Here is the Python code that accomplishes the tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Itera...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # Check if th...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Read data from the provided Excel file and specified sheet\n",
      "df = pd.read_excel('/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 1 - Inventory Planning.xlsx', sheet_name='Invoice Tracker(TK)')\n",
      "\n",
      "# Create a new folder named 'Invoice Tracker(TK)'\n",
      "if not os.path.exists('Invoice Tracker(TK)'):\n",
      "    os.makedirs('Invoice Tracker(TK)')\n",
      "\n",
      "# Initialize variables\n",
      "start_row = 0\n",
      "table_number = 1\n",
      "\n",
      "# Iterate over the DataFrame\n",
      "for i in range(len(df)):\n",
      "    # Check if the row is blank or contains no data\n",
      "    if df.iloc[i].isnull().all():\n",
      "        # Extract the table and save it as a separate CSV file in the folder\n",
      "        table = df.iloc[start_row:i]\n",
      "        table.to_csv(f'Invoice Tracker(TK)/table_{table_number}.csv', index=False)\n",
      "        # Update the start row and table number\n",
      "        start_row = i + 1\n",
      "        table_number += 1\n",
      "\n",
      "# Extract the last table and save it as a separate CSV file in the folder\n",
      "table = df.iloc[start_row:]\n",
      "table.to_csv(f'Invoice Tracker(TK)/table_{table_number}.csv', index=False)...\n",
      "Execution result:\n",
      "\n",
      "Code executed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sheet_to_analyze in sheet_names[-4:]:\n",
    "    print('-'*40,f'\\n {sheet_to_analyze}')\n",
    "    print(run_analysis(sheet_to_analyze))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This threw in the folders everywhere\n",
    "Cost: 0.05 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      "Analyzing Sheet: SKU List\n",
      "Starting analysis for sheet: SKU List\n",
      "Fetching 50xN chunk from sheet: SKU List\n",
      "Fetched chunk from sheet 'SKU List':\n",
      "product, title, Cost, Price, Smart SKU id, Supplier, Menu, Category, Qty Per Case, Share\n",
      "000110001508, Ground Beef 85/15, 15.0, None, =A2*1, Rastelli's, CORE, Beef, 18.0, =sumifs('Take rate per SKU'!DU8:DU89,'Take rate per SKU'!A8:A89,A2)\n",
      "000110003502, Ground Beef 80/20, 15.0, None, 110003502.0, Mission Driven, CORE, Beef, 18.0, =sumifs('Take rate per SKU'!DU9:DU90,'Take rate per SKU'!A9:A90,A3)\n",
      "000110001607, 100% Grass-Fed Ground Beef, 15.0, None, =A4*1, Rastelli's, CORE, Beef, 18.0, =sumifs('T...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_36661/2112633862.py:51: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as separate CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU List'\n",
      "\n",
      "# Read the data from the specified Excel sh...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU List'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'SKU List' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'SKU List')\n",
      "os.makedirs(out...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU List'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'SKU List' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'SKU List')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables based on contiguous blocks of data\n",
      "# Here we assume that the data is continuous and there are no blank rows in the provided chunk\n",
      "# If there were blank rows, we would need to handle that differently\n",
      "\n",
      "# Since the provided data does not have blank rows, we can treat the entire data as one table\n",
      "# If there were multiple tables, we would need to split them based on blank rows\n",
      "\n",
      "# Save the entire data as a CSV file\n",
      "output_file_path = os.path.join(output_folder, 'SKU_List.csv')\n",
      "data.to_csv(output_file_path, index=False)\n",
      "\n",
      "print(f\"Data extracted and saved to {output_file_path}\")...\n",
      "Execution result:\n",
      "Data extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU List/SKU_List.csv\n",
      "\n",
      "Code executed successfully!\n",
      "Data extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU List/SKU_List.csv\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Inbound Receipts\n",
      "Starting analysis for sheet: Inbound Receipts\n",
      "Fetching 50xN chunk from sheet: Inbound Receipts\n",
      "Fetched chunk from sheet 'Inbound Receipts':\n",
      "None, ID, Receipt #, Date, Week, Location, Supplier, PO Number, Order number, SKU code, Category, Description, Price per lbs/ SKU, Order Size, Order unit, Price per unit, Case/ Gaylord size, Case/ Gaylord unit, Order Quantity, Amount, Ordered, Received, Amount received, Status, Receiving Month, Delta, Invoice number, Status invoice, Unit Cost, Semester, Category, Week, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "1.0, =if(isblank(H2),\"\",text(0+A2,\"0000\")&\"_GCPOL\"), 2.0,...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Inbound Receipts'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "d...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Inbound Receipts'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Inbound Receipts' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Inbound Receipts')\n",
      "os.makedirs(output_folde...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Inbound Receipts'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Inbound Receipts' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Inbound Receipts')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Function to identify tables in the DataFrame\n",
      "def identify_tables(df):\n",
      "    # Identify rows that are completely empty\n",
      "    empty_rows = df.isnull().all(axis=1)\n",
      "    \n",
      "    # Split the DataFrame into tables based on empty rows\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in df.iterrows():\n",
      "        if empty_rows[index]:\n",
      "            if current_table:  # If current_table is not empty, save it\n",
      "                tables.append(pd.DataFrame(current_table))\n",
      "                current_table = []  # Reset for the next table\n",
      "        else:\n",
      "            current_table.append(row)\n",
      "    \n",
      "    # Add the last table if it exists\n",
      "    if current_table:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Identify tables in the data\n",
      "tables = identify_tables(data)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Create a filename for each table\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Inbound Receipts'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Inbound Receipts'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: SKU depletions\n",
      "Starting analysis for sheet: SKU depletions\n",
      "Fetching 50xN chunk from sheet: SKU depletions\n",
      "Fetched chunk from sheet 'SKU depletions':\n",
      "Warehouse, Order Type, SKU, Description, Ship Date, Total Qty, DC, Type, Product, Location, DC\n",
      "Goodchopmo, Custom File, 110000907.0, Flat Iron Steaks, 2021-06-30 00:00:00, 13.0, =vlookup(A2,J:K,2,false), =vlookup(B2,J:K,2,false), =iferror(if(match(C2,#REF!,0)>0,\"Product\",\"Other\"),\"Other\"), Goodchopmo, SW MO\n",
      "Goodchopmo, Custom File, 110001003.0, Bavette Steak, 2021-06-30 00:00:00, 14.0, =vlookup(A3,J:K,2,false), =vlookup(B3,J:K,2,false), =iferror(if(match(C3,#REF!,0)>0,\"Product\",\"Other\"),\"Other\")...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU depletions'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "dat...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU depletions'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'SKU depletions' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'SKU depletions...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'SKU depletions'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'SKU depletions' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'SKU depletions')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Function to identify tables in the data\n",
      "def identify_tables(df):\n",
      "    # Identify rows that are not empty\n",
      "    non_empty_rows = df.dropna(how='all')\n",
      "    \n",
      "    # Split the data into tables based on empty rows\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in non_empty_rows.iterrows():\n",
      "        if row.isnull().all():  # Check if the row is completely empty\n",
      "            if current_table:  # If current_table is not empty, save it\n",
      "                tables.append(pd.DataFrame(current_table))\n",
      "                current_table = []  # Reset for the next table\n",
      "        else:\n",
      "            current_table.append(row)\n",
      "    \n",
      "    # Add the last table if it exists\n",
      "    if current_table:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Identify tables in the data\n",
      "tables = identify_tables(data)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68504 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68505 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68586 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68587 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68668 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68669 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68750 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68751 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68832 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68833 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68914 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68915 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68996 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68997 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69078 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69079 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69160 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69161 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69242 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69243 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69324 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69325 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69406 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69407 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69489 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69490 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69571 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69572 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69653 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69654 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69735 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69736 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69818 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69819 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69900 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69901 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69982 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69983 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70064 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70065 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70146 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70147 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70228 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70229 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70310 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70311 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70392 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70393 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70475 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70476 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70557 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70558 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70639 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70640 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70721 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70722 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70803 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70804 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70885 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70886 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70967 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70968 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71049 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71050 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71131 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71132 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71213 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71214 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71295 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71296 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71377 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71378 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71460 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71461 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71542 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71543 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71624 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71625 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71706 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71707 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71788 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71789 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71870 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71871 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71952 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71953 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72034 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72035 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72116 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72117 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72198 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72199 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72280 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72281 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72362 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72363 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72445 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72446 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72527 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72528 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72609 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72610 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72691 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72692 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72773 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72774 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72855 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72856 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72937 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72938 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73019 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73020 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73101 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73102 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73183 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73184 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73265 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73266 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73347 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73348 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU depletions'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU depletions'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Rollling Sales Forecast\n",
      "Starting analysis for sheet: Rollling Sales Forecast\n",
      "Fetching 50xN chunk from sheet: Rollling Sales Forecast\n",
      "Fetched chunk from sheet 'Rollling Sales Forecast':\n",
      "None, Wk0, Wk 1, Wk 2, Wk 3, Wk 4, Wk 5, Wk 6, Wk 7, Wk 8, Wk 9, Wk 10, Wk 11, Wk 12, Wk 13, Wk 14, Wk 15, Wk 16, Wk 17, Wk 18, Wk 19, Wk 20, Wk 21, Wk 22, Wk 23, Wk 24, Wk 25, Wk 26, Wk 27, Wk 28, Wk 29, Wk 30, Wk 31, Wk 32, Wk 33, Wk 34, Wk 35, Wk 36, Wk 37, Wk 38, Wk 39, Wk 40, Wk 41, Wk 42, Wk 43, Wk 44, Wk 45, Wk 46, Wk 47, Wk 48, Wk 49, Wk 50, Wk 51, Wk 52, Wk 53, Wk 54, Wk 55, Wk 56, Wk 57, Wk 58, Wk 59, Wk 60, Wk 61, Wk 62, Wk 63, Wk 64, Wk 65, Wk 66, Wk 67, Wk 68, Wk 69, Wk 70, Wk 71, W...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will first analyze the provided data to identify the tables. Then, we will generate Python code that reads the data from the specified Excel file and sheet, creates a new folder, and saves each identified table as a separate CSV file.\n",
      "\n",
      "### Step 1: Identify Tables\n",
      "\n",
      "From the provided data, we can see that there are several blocks of data separated by rows that contain only `None` values or are completely empty. Each block of data can be considered a table. \n",
      "\n",
      "##...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Rollling Sales Forecast'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Rollling Sales Forecast'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty or contains only None values\n",
      "    if row.isnull().all():\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Create a new folder to save the tables\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Rollling Sales Forecast'\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Rollling Sales Forecast'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Rollling Sales Forecast'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Supply Forecast\n",
      "Starting analysis for sheet: Supply Forecast\n",
      "Fetching 50xN chunk from sheet: Supply Forecast\n",
      "Fetched chunk from sheet 'Supply Forecast':\n",
      "View, None, None, None, None, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Actuals, Ac...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 362054 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 362054 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Share by SKU\n",
      "Starting analysis for sheet: Share by SKU\n",
      "Fetching 50xN chunk from sheet: Share by SKU\n",
      "Fetched chunk from sheet 'Share by SKU':\n",
      "title, Share, None, Category, Share\n",
      "Boneless Skinless Chicken Breasts, =vlookup(A2,'SKU List'!B:J,9,false), None, Beef, =sumifs('SKU List'!J:J,'SKU List'!H:H,D2)\n",
      "Boneless Ribeye Steaks, =vlookup(A3,'SKU List'!B:J,9,false), None, Pork, =sumifs('SKU List'!J:J,'SKU List'!H:H,D3)\n",
      "Thick-Cut Bacon, 12oz, =vlookup(A4,'SKU List'!B:J,9,false), None, Poultry, =sumifs('SKU List'!J:J,'SKU List'!H:H,D4)\n",
      "Ground Beef 80/20, =vlookup(A5,'SKU List'!B:J,9,false), None, Seafood, =sumifs('SKU List'!J:J,'SKU List'!H...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Share by SKU'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "dat...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Share by SKU'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Share by SKU' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Share by SKU')\n",
      "os....\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Share by SKU'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Share by SKU' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Share by SKU')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables in the data\n",
      "# A table is defined as contiguous blocks of data separated by blank rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are NaN)\n",
      "    if row.isnull().all():\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, save it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Share by SKU'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Share by SKU'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Take rate per SKU\n",
      "Starting analysis for sheet: Take rate per SKU\n",
      "Fetching 50xN chunk from sheet: Take rate per SKU\n",
      "Fetched chunk from sheet 'Take rate per SKU':\n",
      "View, None, None, None, None, ='Supply Forecast'!F1, ='Supply Forecast'!G1, ='Supply Forecast'!H1, ='Supply Forecast'!I1, ='Supply Forecast'!J1, ='Supply Forecast'!K1, ='Supply Forecast'!L1, ='Supply Forecast'!M1, ='Supply Forecast'!N1, ='Supply Forecast'!O1, ='Supply Forecast'!P1, ='Supply Forecast'!Q1, ='Supply Forecast'!R1, ='Supply Forecast'!S1, ='Supply Forecast'!T1, ='Supply Forecast'!U1, ='Supply Forecast'!V1, ='Supply Forecast'!W1, ='Supply Forecast'!X1, ='Supply Forecast'!Y1, ='Supply F...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o-mini in organization org-w2JamyIJQNwuaXuQDgZCdN0T on tokens per min (TPM): Limit 200000, Requested 578290. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error during code generation: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o-mini in organization org-w2JamyIJQNwuaXuQDgZCdN0T on tokens per min (TPM): Limit 200000, Requested 578290. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: DC Fcst\n",
      "Starting analysis for sheet: DC Fcst\n",
      "Fetching 50xN chunk from sheet: DC Fcst\n",
      "Fetched chunk from sheet 'DC Fcst':\n",
      "=\"Week \"&R2, BOXES SHIPPED BREAKDOWN, None, None, None, None, MANUAL OVERWRITE, None, None, None, None, None, None, None, None, None, Year, 2023.0\n",
      "Share by DC, Share, Monday, Tuesday, Wednesday, Thursday, Share, Monday, Tuesday, Wednesday, Thursday, None, None, None, None, None, Next week num, 43.0\n",
      "SW MO, <openpyxl.worksheet.formula.ArrayFormula object at 0x15cf6de50>, 0.27, 0.37, 0.31, 0.05, =B3, =C3, =D3, =E3, =F3, None, None, None, None, None, Week Tue, =sumifs('Supply Forecast'!4:4,'Supply F...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `openpyxl` library to read the Excel file and the `pandas` library to handle the data and save it as CSV files. Below is the code that performs the required operations:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "from openpyxl import load_workbook\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_...\n",
      "Extracted code:\n",
      "import os\n",
      "import pandas as pd\n",
      "from openpyxl import load_workbook\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'DC Fcst'\n",
      "\n",
      "# Load the workbook and the specified sheet\n",
      "workbook = load_workbook(excel_file_path, data_only=True)\n",
      "sheet = workbook[sheet_name]\n",
      "\n",
      "# Function to extract tables from the sheet\n",
      "def extract_tables(sheet):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "...\n",
      "Executing the following code:\n",
      "import os\n",
      "import pandas as pd\n",
      "from openpyxl import load_workbook\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'DC Fcst'\n",
      "\n",
      "# Load the workbook and the specified sheet\n",
      "workbook = load_workbook(excel_file_path, data_only=True)\n",
      "sheet = workbook[sheet_name]\n",
      "\n",
      "# Function to extract tables from the sheet\n",
      "def extract_tables(sheet):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for row in sheet.iter_rows(values_only=True):\n",
      "        # Check if the row is empty (all values are None)\n",
      "        if all(cell is None for cell in row):\n",
      "            # If current_table has data, save it as a table\n",
      "            if current_table:\n",
      "                tables.append(current_table)\n",
      "                current_table = []\n",
      "        else:\n",
      "            current_table.append(row)\n",
      "    \n",
      "    # Add the last table if it exists\n",
      "    if current_table:\n",
      "        tables.append(current_table)\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Extract tables from the sheet\n",
      "tables = extract_tables(sheet)\n",
      "\n",
      "# Create a new folder to save the CSV files\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'DC Fcst')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Convert the table to a DataFrame\n",
      "    df = pd.DataFrame(table)\n",
      "    \n",
      "    # Define the CSV file name\n",
      "    csv_file_name = f'table_{i + 1}.csv'\n",
      "    csv_file_path = os.path.join(output_folder, csv_file_name)\n",
      "    \n",
      "    # Save the DataFrame to a CSV file\n",
      "    df.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68504 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68505 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68586 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68587 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68668 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68669 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68750 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68751 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68832 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68833 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68914 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68915 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68996 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C68997 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69078 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69079 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69160 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69161 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69242 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69243 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69324 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69325 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69406 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69407 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69489 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69490 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69571 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69572 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69653 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69654 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69735 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69736 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69818 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69819 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69900 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69901 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69982 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C69983 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70064 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70065 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70146 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70147 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70228 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70229 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70310 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70311 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70392 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70393 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70475 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70476 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70557 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70558 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70639 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70640 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70721 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70722 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70803 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70804 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70885 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70886 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70967 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C70968 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71049 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71050 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71131 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71132 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71213 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71214 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71295 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71296 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71377 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71378 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71460 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71461 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71542 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71543 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71624 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71625 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71706 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71707 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71788 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71789 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71870 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71871 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71952 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C71953 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72034 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72035 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72116 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72117 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72198 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72199 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72280 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72281 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72362 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72363 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72445 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72446 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72527 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72528 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72609 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72610 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72691 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72692 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72773 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72774 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72855 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72856 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72937 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C72938 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73019 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73020 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73101 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73102 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73183 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73184 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73265 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73266 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73347 is marked as a date but the serial value 310001902.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n",
      "/Users/ajay/miniconda3/envs/crustdata/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:223: UserWarning: Cell C73348 is marked as a date but the serial value 310002008.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution result:\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/DC Fcst'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/DC Fcst'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Sheet99\n",
      "Starting analysis for sheet: Sheet99\n",
      "Fetching 50xN chunk from sheet: Sheet99\n",
      "Fetched chunk from sheet 'Sheet99':\n",
      "5778.0, 6402.0, 6584.0, 6159.0, 6491.0, 6492.0, 6916.0, 5904.0, 5864.0, 6148.0, 6507.223323, 6401.169147999999, 6698.948286, 6729.51055, 6733.489131, 6685.658600999999, 7398.131401000001, 7516.267332000001, 7581.4740059999995, 7604.678448, 7965.889652, 8099.334318000001, 8193.715719, 8274.442824000002, 8532.236854, 8676.624462, 8780.585454, 8872.907608000001, 9080.907068000002, 9233.223610000001, 8021.093799, 8088.012507, 9510.996492000002, 10000.0, =AH1*1.01, =AI1*1.01, =AJ1*1.01, =AK1*1.01, =A...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet99'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd....\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet99'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# A table is defined as contiguous non-empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet99'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# A table is defined as contiguous non-empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected data for a table\n",
      "            tables.append(pd.DataFrame(current_table))  # Save the current table\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)  # Add the row to the current table\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Create a new folder named 'Sheet99' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Sheet99')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet99'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet99'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Sheet98\n",
      "Starting analysis for sheet: Sheet98\n",
      "Fetching 50xN chunk from sheet: Sheet98\n",
      "Fetched chunk from sheet 'Sheet98':\n",
      "midwest, None, ='Supply Forecast'!DI3, ='Supply Forecast'!DJ3, ='Supply Forecast'!DK3, ='Supply Forecast'!DL3, ='Supply Forecast'!DM3, ='Supply Forecast'!DN3, ='Supply Forecast'!DO3, ='Supply Forecast'!DP3, ='Supply Forecast'!DQ3, ='Supply Forecast'!DR3, ='Supply Forecast'!DS3, ='Supply Forecast'!DT3, ='Supply Forecast'!DU3, ='Supply Forecast'!DV3, ='Supply Forecast'!DW3, ='Supply Forecast'!DX3, ='Supply Forecast'!DY3, ='Supply Forecast'!DZ3, ='Supply Forecast'!EA3, ='Supply Forecast'!EB3, ='Sup...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as CSV files. Below is the code that fulfills the requirements:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet98'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data =...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet98'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all el...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Sheet98'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are None)\n",
      "    if row.isnull().all():\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Create a new folder named 'Sheet98' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Sheet98')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet98'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet98'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: DC Inventory\n",
      "Starting analysis for sheet: DC Inventory\n",
      "Fetching 50xN chunk from sheet: DC Inventory\n",
      "Fetched chunk from sheet 'DC Inventory':\n",
      "Date updated, 2023-10-14 00:00:00, Morning, None, None, None\n",
      "None, None, None, CURRENT INVENTORY, None, None\n",
      "sk_recipe, alternative code, title, NW, GN, GK\n",
      "110000303.0, 110000303.0, Boneless Ribeye Steaks, =if(sumifs(NW!D:D,NW!B:B,A4)=0,sumifs(NW!D:D,NW!B:B,B4),sumifs(NW!D:D,NW!B:B,A4)), =if(sumifs(RLS!E:E,RLS!A:A,A4,RLS!C:C,\"Vineland North\")+sumifs(RLS!E:E,RLS!A:A,A4,RLS!C:C,\"RLS Newfield FC\")=0,sumifs(RLS!E:E,RLS!A:A,B4,RLS!C:C,\"Vineland North\")+sumifs(RLS!E:E,RLS!A:A,B4,RLS!C:C,\"RLS Newfield ...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'DC Inventory'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd....\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'DC Inventory'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'DC Inventory' inside the specified directory\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 -...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'DC Inventory'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'DC Inventory' inside the specified directory\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/DC Inventory'\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables by splitting the data on empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are None)\n",
      "    if row.isnull().all():\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/DC Inventory'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/DC Inventory'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: NW\n",
      "Starting analysis for sheet: NW\n",
      "Fetching 50xN chunk from sheet: NW\n",
      "Fetched chunk from sheet 'NW':\n",
      "PRODUCT, CUSTOMER REF PRODUCT, PRODUCT DESCRIPTION, ON HAND (ACTL), COMMIT QUAN, AVAIL QUAN, EXPECT QUAN, ON HAND PALLET, ON HAND PALLET, ON HAND PALLET, AVAIL PALLET\n",
      "GC201, CD ICECB4, DRY ICE BLOCK 5 LBS, 4560.0, 0.0, 4560.0, 39600.0, 12.0, 0 + 4560c, 12.0, 12.0, =match(B2,'DC Inventory'!A:A,0)\n",
      "GC203, BOX-100002, LARGE BOX - 13 1/2 x 13 1/2 x 18 1/2, 3022.0, 0.0, 3022.0, 0.0, 13.0, 302 + 2c, 12.08, 13.0, =match(B3,'DC Inventory'!A:A,0)\n",
      "GC204, LNR-100003, LARGE LINER A, 2820.0, 0.0, 2820.0, 0.0,...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'NW'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'NW'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'NW' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'NW')\n",
      "os.makedirs(output_folder, exist_...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'NW'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'NW' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'NW')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables based on contiguous blocks of data\n",
      "# Here we assume that the data is well-formed and there are no completely empty rows\n",
      "# We will use the first column to identify breaks in the data\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are NaN)\n",
      "    if row.isnull().all():\n",
      "        # If we encounter an empty row and current_table is not empty, save the current table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        # Append the current row to the current table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/NW'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/NW'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: RLS\n",
      "Starting analysis for sheet: RLS\n",
      "Fetching 50xN chunk from sheet: RLS\n",
      "Fetched chunk from sheet 'RLS':\n",
      "Item Code, Item, Warehouse, Group, On Hand, On Order, On Hold, Available\n",
      "110001102.0, Beef Flank Steak, USDA 100% Grass Fed ABF, 4 x 6 oz, RLS Newfield FC, Goods, 1213.0, 0.0, 0.0, 1213.0\n",
      "110000204.0, Beef Filet Mignon, USDA 100% Grass Fed ABF, 2 x 6 oz, RLS Newfield FC, Goods, 3486.0, 0.0, 62.0, 3424.0\n",
      "110003908.0, Beef Hot Dogs, USDA 100% Grass Fed Domestic ABF, 2 x 1 lb, RLS Newfield FC, Goods, 1743.0, 0.0, 0.0, 1743.0\n",
      "110003300004.0, Beef Ribeye Steak, USDA 100% Grass Fed ABF, 2 x 10 oz, RLS...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as separate CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'RLS'\n",
      "\n",
      "# Create a new folder named 'RLS' inside the ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'RLS'\n",
      "\n",
      "# Create a new folder named 'RLS' inside the specified directory\n",
      "output_folder = os.path.join(os.path.dirname(excel_file_path), 'RLS')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'RLS'\n",
      "\n",
      "# Create a new folder named 'RLS' inside the specified directory\n",
      "output_folder = os.path.join(os.path.dirname(excel_file_path), 'RLS')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables based on contiguous blocks of data\n",
      "# Here we assume that the data is well-formed and does not contain any blank rows\n",
      "# If there are blank rows, we can use them to split the data into tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are NaN)\n",
      "    if row.isnull().all():\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis/Data/RLS'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis/Data/RLS'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Total Inventory\n",
      "Starting analysis for sheet: Total Inventory\n",
      "Fetching 50xN chunk from sheet: Total Inventory\n",
      "Fetched chunk from sheet 'Total Inventory':\n",
      "View, None, None, ='Supply Forecast'!CK1, ='Supply Forecast'!CL1, ='Supply Forecast'!CM1, ='Supply Forecast'!CN1, ='Supply Forecast'!CO1, ='Supply Forecast'!CP1, ='Supply Forecast'!CQ1, ='Supply Forecast'!CR1, ='Supply Forecast'!CS1, ='Supply Forecast'!CT1, ='Supply Forecast'!CU1, ='Supply Forecast'!CV1, ='Supply Forecast'!CW1, ='Supply Forecast'!CX1, ='Supply Forecast'!CY1, ='Supply Forecast'!CZ1, ='Supply Forecast'!DA1, ='Supply Forecast'!DB1, ='Supply Forecast'!DC1, ='Supply Forecast'!DD1, ='...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as separate CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Total Inventory'\n",
      "\n",
      "# Read the data from the specified E...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Total Inventory'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'Total Inventory' inside the specified directory\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Compa...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Total Inventory'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'Total Inventory' inside the specified directory\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Total Inventory'\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Function to identify tables in the data\n",
      "def identify_tables(data):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in data.iterrows():\n",
      "        # Check if the row is completely empty or contains only None values\n",
      "        if row.isnull().all():\n",
      "            if current_table:  # If current_table is not empty, save it\n",
      "                tables.append(pd.DataFrame(current_table))\n",
      "                current_table = []  # Reset for the next table\n",
      "        else:\n",
      "            current_table.append(row.tolist())\n",
      "    \n",
      "    # If there's any remaining data in current_table, add it as the last table\n",
      "    if current_table:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Identify tables in the data\n",
      "tables = identify_tables(data)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Total Inventory'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management/Total Inventory'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Packaging depletions\n",
      "Starting analysis for sheet: Packaging depletions\n",
      "Fetching 50xN chunk from sheet: Packaging depletions\n",
      "Fetched chunk from sheet 'Packaging depletions':\n",
      "Location, Year, Week, SKU, Description, Quantity, None, None, None, Item, Units per pallet, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "SW FL, 2022.0, 35.0, LNR-100003, LARGE LINER A, 58.0, None, None, None, BOX-100001, 250.0\n",
      "SW MO, 2022.0, 35.0, LNR-100003, LARGE LINER A, 134.0, None, None, None, BOX-100002, 313.0\n",
      "SW NV, 2022.0, 35.0, LNR-100003, LARGE LINER A, 58.0, None, None, None, BOX-100003, 1350.0\n",
      "SW PA, 2022.0, 35.0, LNR-100003, LARGE LI...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as separate CSV files. Below is the code that fulfills your requirements:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Packaging depletions'\n",
      "\n",
      "# Read the data from the sp...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Packaging depletions'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Packaging depletions' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Pa...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Packaging depletions'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Packaging depletions' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Packaging depletions')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables based on contiguous blocks of data\n",
      "# Here we assume that a table is defined as a block of non-empty rows\n",
      "# We will drop rows that are completely empty\n",
      "data_cleaned = data.dropna(how='all')\n",
      "\n",
      "# Initialize a list to hold the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the cleaned data to identify tables\n",
      "for index, row in data_cleaned.iterrows():\n",
      "    if row.isnull().all():  # If the row is completely empty, we consider it a separator\n",
      "        if current_table:  # If we have collected rows for a table, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Packaging depletions'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Packaging depletions'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Hf market SKU breakdown\n",
      "Starting analysis for sheet: Hf market SKU breakdown\n",
      "Fetching 50xN chunk from sheet: Hf market SKU breakdown\n",
      "Fetched chunk from sheet 'Hf market SKU breakdown':\n",
      "49.99, Good Chop Sampler Box, 271.0, =C1/(C$1+C$7+C$13+C$21+C$27+C$33), None, None, None, None, None, Share, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "000310001704, Boneless Skinless Chicken Breasts, 271.0, None, None, None, None, None, Box, Forecasted, Actual, Price, Nb of boxes\n",
      "000210001804, Thick-Cut Bacon, 271.0, None, None, None, None, None, Good Chop Sampler Box, 0.55, =vlookup(I3,B:D,3,false), 49.99, =vlookup(I3,B:D,2,false)\n",
      "000110002901, Ranch Ste...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables based on the provided data, and save each table as a separate CSV file. Below is the code that performs these tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Hf market SKU breakdown'\n",
      "\n",
      "# R...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Hf market SKU breakdown'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on blank rows\n",
      "# A table is defined as a contiguous block of non-empty rows\n",
      "tables = []\n",
      "current_table = [...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Hf market SKU breakdown'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Identify tables by splitting the data on blank rows\n",
      "# A table is defined as a contiguous block of non-empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are None or NaN)\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it as a table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        # Append the non-empty row to the current table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Create a new folder to save the tables\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Hf market SKU breakdown')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the CSV file name\n",
      "    csv_file_name = f'table_{i + 1}.csv'\n",
      "    csv_file_path = os.path.join(output_folder, csv_file_name)\n",
      "    \n",
      "    # Save the table to a CSV file\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Hf market SKU breakdown'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Hf market SKU breakdown'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Orders shipped\n",
      "Starting analysis for sheet: Orders shipped\n",
      "Fetching 50xN chunk from sheet: Orders shipped\n",
      "Fetched chunk from sheet 'Orders shipped':\n",
      "Location, Ship Date, Nb orders, DC\n",
      "Goodchopmo, 2021-06-30 00:00:00, 10.0, =vlookup(A2,'SKU depletions'!J:K,2,false)\n",
      "Goodchopmo, 2021-07-08 00:00:00, 14.0, =vlookup(A3,'SKU depletions'!J:K,2,false)\n",
      "Goodchopmo, 2021-07-13 00:00:00, 36.0, =vlookup(A4,'SKU depletions'!J:K,2,false)\n",
      "Goodchopmo, 2021-07-16 00:00:00, 2.0, =vlookup(A5,'SKU depletions'!J:K,2,false)\n",
      "Goodchopmo, 2021-07-20 00:00:00, 63.0, =vlookup(A6,'SKU depletions'!J:K,2,false)\n",
      "Goodchopmo, 2021-07-28 00:00:00, 41.0, =vlookup(A7,'SKU deple...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Orders shipped'\n",
      "\n",
      "# Read the data from the specified Excel sheet...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Orders shipped'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Orders shipped' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Orders shipped...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Orders shipped'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Orders shipped' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Orders shipped')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables based on contiguous blocks of data\n",
      "# Here we assume that the data is already in a single block as provided\n",
      "# If there were blank rows, we would need to handle that differently\n",
      "\n",
      "# Save the entire data as a single CSV file\n",
      "output_file_path = os.path.join(output_folder, 'orders_shipped.csv')\n",
      "data.to_csv(output_file_path, index=False)\n",
      "\n",
      "print(f\"Data has been extracted and saved to {output_file_path}\")...\n",
      "Execution result:\n",
      "Data has been extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Orders shipped/orders_shipped.csv\n",
      "\n",
      "Code executed successfully!\n",
      "Data has been extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Orders shipped/orders_shipped.csv\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Replen tool\n",
      "Starting analysis for sheet: Replen tool\n",
      "Fetching 50xN chunk from sheet: Replen tool\n",
      "Fetched chunk from sheet 'Replen tool':\n",
      "None, None, None, SW MO, None, SW NV, None, None, None, None, None, SW PA, None, None, None, None, None, SW FL, None, None, None, None, None, RLS MO, None, None, None, None, None, RLS NJ, None, None, None, None, None, RLS TX, None, None, None, None, None, GL CA, None, None, None, None, None, CAS GA, None, None, None, None, None, CAS KY, None, None, None, None, None\n",
      "product, title, Smart SKU id, 2023-07-04 00:00:00, Qty Per Case, =D2, 2023-08-08 00:00:00, Nb of cases, Nb of units, % of MO invento...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will create a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that implements these requirements:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen tool'\n",
      "\n",
      "# Read the data from the specified Excel ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen tool'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'Replen tool' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Replen ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen tool'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Create a new folder named 'Replen tool' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Replen tool')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Function to identify tables in the data\n",
      "def extract_tables(data):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in data.iterrows():\n",
      "        # Check if the row is empty (all elements are None)\n",
      "        if row.isnull().all():\n",
      "            # If current_table is not empty, save it as a table\n",
      "            if current_table:\n",
      "                tables.append(pd.DataFrame(current_table))\n",
      "                current_table = []\n",
      "        else:\n",
      "            # Append the current row to the current table\n",
      "            current_table.append(row.tolist())\n",
      "    \n",
      "    # Add the last table if it exists\n",
      "    if current_table:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Extract tables from the data\n",
      "tables = extract_tables(data)\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen tool'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen tool'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: CP Forecast (2023)\n",
      "Starting analysis for sheet: CP Forecast (2023)\n",
      "Fetching 50xN chunk from sheet: CP Forecast (2023)\n",
      "Fetched chunk from sheet 'CP Forecast (2023)':\n",
      "None, FY2023 Forecast, None, None, None, None, None, None, Quarter, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, Week Of, 2023-01-02 00:00:00, 2023-01-09 00:00:00, 2023-01-16 00:00:00, 2023-01-23 00:00:00, ...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will first analyze the provided data to identify the tables. The data appears to be structured in a way that suggests multiple tables, each separated by blank rows. \n",
      "\n",
      "### Step 1: Identify Tables\n",
      "From the provided data, we can see that there are several blocks of data that can be considered as tables. Each table is separated by rows that contain `None` values or are completely empty. \n",
      "\n",
      "### Step 2: Generate Python Code\n",
      "The following Python code will read the sp...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'CP Forecast (2023)'\n",
      "\n",
      "# Create a new folder for saving the extracted tables\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'CP Forecast (2023)')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(excel_file_path, sheet_name=sheet_na...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'CP Forecast (2023)'\n",
      "\n",
      "# Create a new folder for saving the extracted tables\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'CP Forecast (2023)')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Initialize variables to track tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the rows of the DataFrame\n",
      "for index, row in df.iterrows():\n",
      "    # Check if the row is completely empty or contains only None values\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it as a table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset current table\n",
      "    else:\n",
      "        # Append the current row to the current table\n",
      "        current_table.append(row.tolist())\n",
      "\n",
      "# If there's any remaining data in current_table, save it as a table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 3 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/CP Forecast (2023)'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 3 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/CP Forecast (2023)'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Dry Ice Forecast\n",
      "Starting analysis for sheet: Dry Ice Forecast\n",
      "Fetching 50xN chunk from sheet: Dry Ice Forecast\n",
      "Fetched chunk from sheet 'Dry Ice Forecast':\n",
      "None, FY2023 Forecast, None, None, None, Quarter, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q3, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, Q4, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, Week Of, 2023-01-02 00:00:00, 2023-01-09 00:00:00, 2023-01-16 00:00:00, 2023-01-23 00:00:00, 2023-01-30 00:00:00, 2023-02-06 00:00:00, 2023-02-13 00:00:00, 202...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Dry Ice Forecast'\n",
      "\n",
      "# Read the data from the specified Excel she...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Dry Ice Forecast'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Function to identify tables in the data\n",
      "def identify_tables(data):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in data.iterrows():...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Dry Ice Forecast'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Function to identify tables in the data\n",
      "def identify_tables(data):\n",
      "    tables = []\n",
      "    current_table = []\n",
      "    \n",
      "    for index, row in data.iterrows():\n",
      "        # Check if the row is completely empty or contains only None values\n",
      "        if row.isnull().all():\n",
      "            if current_table:  # If we have collected a table, save it\n",
      "                tables.append(pd.DataFrame(current_table))\n",
      "                current_table = []  # Reset for the next table\n",
      "        else:\n",
      "            current_table.append(row.tolist())\n",
      "    \n",
      "    # If there's any remaining data in current_table, add it as the last table\n",
      "    if current_table:\n",
      "        tables.append(pd.DataFrame(current_table))\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Identify tables in the data\n",
      "tables = identify_tables(data)\n",
      "\n",
      "# Create a new folder to save the CSV files\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Dry Ice Forecast')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Generate a filename for each table\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 7 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 7 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Credit Requests\n",
      "Starting analysis for sheet: Credit Requests\n",
      "Fetching 50xN chunk from sheet: Credit Requests\n",
      "Fetched chunk from sheet 'Credit Requests':\n",
      "Entry Date, Vendor, Amount, Reason, Date Requested, Date Paid, PO that Credit was applied to (If Applicable), External Contact, External Email Address, Account, Comment\n",
      "2022-09-30 00:00:00, Smart Warehousing, 6568.13, Invoice 382964, 2022-09-28 00:00:00, None, None, Alan Rust, alan.rust@smartwarehousing.com, PnP, Should be 20k, should be issued W01-2023\n",
      "2022-09-30 00:00:00, Smart Warehousing, 27142.0, Invoice 376500, 2022-09-30 00:00:00, None, None, Alan Rust, alan.rust@smartwarehousing.com, COG...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as separate CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Credit Requests'\n",
      "\n",
      "# Read the data from the spec...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Credit Requests'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by splitting the data on rows with all None values\n",
      "# We will drop rows that are completely empty\n",
      "data_cleaned = data.dropna(how='all')\n",
      "\n",
      "# Create a ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Credit Requests'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by splitting the data on rows with all None values\n",
      "# We will drop rows that are completely empty\n",
      "data_cleaned = data.dropna(how='all')\n",
      "\n",
      "# Create a new folder named 'Credit Requests' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Credit Requests')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Initialize a list to hold the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the cleaned data to identify tables\n",
      "for index, row in data_cleaned.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected a table, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, save it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Credit Requests'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Credit Requests'\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Replen Orders\n",
      "Starting analysis for sheet: Replen Orders\n",
      "Fetching 50xN chunk from sheet: Replen Orders\n",
      "Fetched chunk from sheet 'Replen Orders':\n",
      "Ship from, Ship To State, Receipt No., Carrier, Purchase order, Customer Reference, Type, SKU, Total Qty, Status, Reception status, Create Date, Arrival Date, Confirm Date, Comment, None, Date\n",
      "RLS NJ, GL CA, TRN005, WARP, TRN005, Manual Order-B2B, Manual Order-B2B, 210001804.0, 5184.0, Complete, In transit, 2023-10-04 00:00:00, 2023-10-14 00:00:00, 2023-10-14 00:00:00, Bacon transfer from RLS in NJ to GreenLeaf in CA, None, =(left(right(M2,5),2)&\"/\"&right(M2,2)&\"/\"&left(M2,4))*1\n",
      "SW MO, RLS NJ, 1...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify tables, and save them as CSV files. Below is the code that performs these operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen Orders'\n",
      "\n",
      "# Read the data from the specified Excel she...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen Orders'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Replen Orders' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Replen Orders')\n",
      "...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'Replen Orders'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Create a new folder named 'Replen Orders' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'Company 2 - Supply Management', 'Replen Orders')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# We will assume that a table is separated by rows with all NaN values\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected rows for a table\n",
      "            tables.append(pd.DataFrame(current_table))  # Save the current table\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)  # Add the row to the current table\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen Orders'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen Orders'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: COGS\n",
      "Starting analysis for sheet: COGS\n",
      "Fetching 50xN chunk from sheet: COGS\n",
      "Fetched chunk from sheet 'COGS':\n",
      "=IFERROR(__xludf.DUMMYFUNCTION(\"IMPORTRANGE(\"\"https://docs.google.com/spreadsheets/d/1TAqxa7sotOCRpB5nQQFZbr-1fuuMq6tf8ttfmCUrtF0/edit#gid=204679300\"\",\"\"COGS!A1:o10000\"\")\"),\"#REF!\")...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will create a Python script that uses the `pandas` library to read the Excel file, identify tables based on contiguous blocks of data, and save each table as a separate CSV file. Below is the Python code that performs these tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'COGS'\n",
      "...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'COGS'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Function to identify tables based on contiguous blocks of data\n",
      "def extract_tables(data):\n",
      "    # Create a list to hold the tables\n",
      "    tables = []\n",
      "    \n",
      "    # Identify rows with ...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the sheet name\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
      "sheet_name = 'COGS'\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Function to identify tables based on contiguous blocks of data\n",
      "def extract_tables(data):\n",
      "    # Create a list to hold the tables\n",
      "    tables = []\n",
      "    \n",
      "    # Identify rows with any data\n",
      "    non_empty_rows = data.dropna(how='all')\n",
      "    \n",
      "    # Split the data into tables based on empty rows\n",
      "    for _, group in non_empty_rows.groupby((non_empty_rows.isnull().all(axis=1)).cumsum()):\n",
      "        if not group.empty:\n",
      "            tables.append(group)\n",
      "    \n",
      "    return tables\n",
      "\n",
      "# Extract tables from the data\n",
      "tables = extract_tables(data)\n",
      "\n",
      "# Create a new folder named 'COGS' inside the specified directory\n",
      "output_folder = os.path.join(os.getcwd(), 'COGS')\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Save each identified table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to '{output_folder}'\")...\n",
      "Execution result:\n",
      "Extracted 0 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/COGS'\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 0 tables and saved them to '/Users/ajay/Documents/Atomic/inventory_analysis_2/COGS'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 2 - Supply Management.xlsx'\n",
    "workbook_name = os.path.splitext(os.path.basename(file_path))[0]  # Get workbook name without extension\n",
    "workbook = load_workbook(file_path, data_only=False, read_only=True)\n",
    "sheet_names = [sheet_name for sheet_name in workbook.sheetnames if not workbook[sheet_name].sheet_state == 'hidden']\n",
    "\n",
    "# Create a base directory with the workbook name\n",
    "base_dir = os.path.join(os.getcwd(), workbook_name)\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name not in sheet_names:\n",
    "        log_message = f\"Sheet '{sheet_name}' doesn't exist. Available sheets: {sheet_names}\"\n",
    "        print(log_message)\n",
    "        return log_message\n",
    "    print(f\"Fetching 50xN chunk from sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    print(f\"Fetched chunk from sheet '{sheet_name}':\\n{formatted_chunk[:500]}...\")  # Show first 500 chars for brevity\n",
    "    return formatted_chunk\n",
    "\n",
    "# Function to analyze a chunk and generate code\n",
    "import re\n",
    "\n",
    "def analyze_and_generate_code(sheet_name: str, chunk: str) -> str:\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present in the chunk. Tables are defined as contiguous blocks of data separated by blank rows or rows with no data.\n",
    "    2. Generate Python code that:\n",
    "       - Reads data from the provided Excel file ('{file_path}') and specified sheet ('{sheet_name}').\n",
    "       - Creates a new folder named '{sheet_name}' inside the folder '{workbook_name}' in the current working directory.\n",
    "       - Extracts each identified table based on the above definition and saves it as a separate CSV file in the folder.\n",
    "    3. Do not make any assumptions about the meaning or functionality of specific columns unless explicitly mentioned.\n",
    "    \"\"\"\n",
    "    print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    try:\n",
    "        response = llm.invoke(llm_input)\n",
    "        generated_response = response.content  # Extract string content\n",
    "        print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "        # Extract Python code block using regex\n",
    "        match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "        if match:\n",
    "            generated_code = match.group(1).strip()  # Extract the Python code\n",
    "            print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "            return generated_code\n",
    "        else:\n",
    "            print(\"No valid Python code block found in the LLM response.\")\n",
    "            return \"Error: No valid Python code block found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code generation: {e}\")\n",
    "        return f\"Error during code generation: {e}\"\n",
    "\n",
    "# Function to execute generated Python code\n",
    "def execute_code(code: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executing the following code:\\n{code}...\")  # Log the extracted code\n",
    "        repl_tool = PythonREPLTool()\n",
    "        result = repl_tool.run(code)\n",
    "        print(\"Execution result:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"Execution Error: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return error_message\n",
    "\n",
    "# Main execution process\n",
    "def run_analysis(sheet_name):\n",
    "    print(f\"Starting analysis for sheet: {sheet_name}\")\n",
    "    \n",
    "    # Step 1: Fetch sheet chunk\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    if \"doesn't exist\" in chunk:\n",
    "        print(f\"Error: {chunk}\")\n",
    "        return chunk\n",
    "\n",
    "    # Step 2: Analyze and generate code\n",
    "    print(\"Analyzing the data and generating Python code...\")\n",
    "    code = analyze_and_generate_code(sheet_name, chunk)\n",
    "    if \"Error during code generation\" in code:\n",
    "        return code\n",
    "\n",
    "    # Step 3: Attempt to execute code with retries\n",
    "    retry = 10\n",
    "    while retry > 0:\n",
    "        result = execute_code(code)\n",
    "        if \"Execution Error\" not in result:\n",
    "            print(\"Code executed successfully!\")\n",
    "            return result  # Successful execution\n",
    "\n",
    "        # Provide feedback and request fixes\n",
    "        error_message = f\"\\n\\nError Message: {result}\"\n",
    "        print(f\"Retrying... ({10 - retry + 1}/10)\")\n",
    "        print(f\"Requesting LLM to fix the code. Error: {error_message}\")\n",
    "        code = analyze_and_generate_code(sheet_name, chunk + error_message)\n",
    "        retry -= 1\n",
    "\n",
    "    print(\"Exceeded maximum retries. Could not execute code successfully.\")\n",
    "    return \"Failed after multiple retries.\"\n",
    "\n",
    "# Run the workflow for each sheet\n",
    "for sheet_to_analyze in sheet_names:\n",
    "    # Create a folder for each sheet under the workbook folder\n",
    "    sheet_folder = os.path.join(base_dir, sheet_to_analyze)\n",
    "    os.makedirs(sheet_folder, exist_ok=True)\n",
    "    print('-'*40, f'\\nAnalyzing Sheet: {sheet_to_analyze}')\n",
    "    print(run_analysis(sheet_to_analyze))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is getting written into the right path\n",
    "0.06 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      "Analyzing Sheet: Product\n",
      "Starting analysis for sheet: Product\n",
      "Fetching 50xN chunk from sheet: Product\n",
      "Fetched chunk from sheet 'Product':\n",
      "SKU, Planning SKU, Category, Collection, Product Name, Priority, UPC, MSRP, COGS, Inbound, Ships By, Total On-Hand, Inventory Value, Supplier 1, Supplier 2, SKU MOQ , Collection MOQ , Lead Time, Billed Weight (lbs), Units Per MC, Weight (oz), weight lbs, next_gen_sku, HTS Codes, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "Bla-VerA-M08, None, Watches, Marble Auto, =iferror(VLOOKUP(A2,Sales!Z:AE,6,false),\"\"), A, 749524585012.0, =...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will create a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the 'Product' sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that performs the required tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/invento...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Product'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet\n",
      "df = pd.read_excel(excel_file_path, sheet...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Product'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet\n",
      "df = pd.read_excel(excel_file_path, sheet_name='Product')\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# We will assume that a table is separated by rows that are completely empty\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected data for a table\n",
      "            tables.append(pd.DataFrame(current_table))  # Save the current table\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)  # Add the row to the current table\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_directory, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_directory}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Product'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Product'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Watches (Master)\n",
      "Starting analysis for sheet: Watches (Master)\n",
      "Fetching 50xN chunk from sheet: Watches (Master)\n",
      "Fetched chunk from sheet 'Watches (Master)':\n",
      "None, None, None, None, None, L30 Shopify Performance, None, None, None, Inventory On Hand, None, None, None, None, None, None, Running Inventory Forecast, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, No...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 146524 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 146524 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Jewelry (Master)\n",
      "Starting analysis for sheet: Jewelry (Master)\n",
      "Fetching 50xN chunk from sheet: Jewelry (Master)\n",
      "Fetched chunk from sheet 'Jewelry (Master)':\n",
      "None, None, None, None, None, L30 Shopify Performance, None, None, None, Inventory On Hand, None, None, None, Past 30 days, None, None, None, Running Inventory Forecast, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, ...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 144702 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 144702 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Forecast\n",
      "Starting analysis for sheet: Forecast\n",
      "Fetching 50xN chunk from sheet: Forecast\n",
      "Fetched chunk from sheet 'Forecast':\n",
      "None, None, 2023-12-01 00:00:00, =IFERROR(__xludf.DUMMYFUNCTION(\"IMPORTRANGE(\"\"https://docs.google.com/spreadsheets/d/1YjtZR6dATSE6wzSEtkjpJRxOoRS6mnSVuv1PNDNuGmQ/edit#gid=1128967832\"\",\"\"2024 category forecasting!C4:N4\"\")\"),45292.0), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),45323.0), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),45352.0), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),45383.0), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),45413.0), =IFERROR(__xlu...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To achieve the tasks outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the 'Forecast' sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that accomplishes this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Com...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet in the Excel file\n",
      "df = pd.read_excel(exc...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet in the Excel file\n",
      "df = pd.read_excel(excel_file_path, sheet_name='Forecast', header=None)\n",
      "\n",
      "# Initialize a list to hold the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the rows of the DataFrame\n",
      "for index, row in df.iterrows():\n",
      "    # Check if the row is empty (all elements are None)\n",
      "    if row.isnull().all():\n",
      "        # If current_table has data, save it to tables and reset current_table\n",
      "        if current_table:\n",
      "            tables.append(current_table)\n",
      "            current_table = []\n",
      "    else:\n",
      "        # Append the current row to the current_table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(current_table)\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Create a DataFrame from the table\n",
      "    table_df = pd.DataFrame(table)\n",
      "    # Define the output CSV file path\n",
      "    csv_file_path = os.path.join(output_directory, f'table_{i + 1}.csv')\n",
      "    # Save the DataFrame to a CSV file\n",
      "    table_df.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_directory}'.\")...\n",
      "Execution result:\n",
      "Extracted 14 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 14 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Carry (Master)\n",
      "Starting analysis for sheet: Carry (Master)\n",
      "Fetching 50xN chunk from sheet: Carry (Master)\n",
      "Fetched chunk from sheet 'Carry (Master)':\n",
      "None, None, None, None, None, L30 Shopify Performance, None, None, None, Inventory On Hand, None, Running Inventory Forecast, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, Past 30 days, ...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To achieve the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, extract the tables, and save them as separate CSV files. The script will identify contiguous blocks of data separated by blank rows and save each block as a separate CSV file.\n",
      "\n",
      "Here's the Python code to accomplish this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Invent...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Carry (Master)'\n",
      "\n",
      "# Define the output directory\n",
      "output_dir = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Carry (Master)'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, she...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Carry (Master)'\n",
      "\n",
      "# Define the output directory\n",
      "output_dir = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Carry (Master)'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Initialize variables to store tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the rows of the DataFrame\n",
      "for index, row in df.iterrows():\n",
      "    # Check if the row is completely empty\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it to tables\n",
      "        if current_table:\n",
      "            tables.append(current_table)\n",
      "            current_table = []  # Reset current_table for the next table\n",
      "    else:\n",
      "        # Append the current row to the current_table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(current_table)\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Create a DataFrame from the table\n",
      "    table_df = pd.DataFrame(table)\n",
      "    \n",
      "    # Define the output CSV file name\n",
      "    csv_file_name = f'table_{i + 1}.csv'\n",
      "    csv_file_path = os.path.join(output_dir, csv_file_name)\n",
      "    \n",
      "    # Save the DataFrame to a CSV file\n",
      "    table_df.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_dir}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Carry (Master)'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Carry (Master)'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Straps (Master)\n",
      "Starting analysis for sheet: Straps (Master)\n",
      "Fetching 50xN chunk from sheet: Straps (Master)\n",
      "Fetched chunk from sheet 'Straps (Master)':\n",
      "None, None, None, None, None, L30 Shopify Performance, None, None, None, Inventory On Hand, None, None, None, None, None, Running Inventory Forecast, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, No...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 147819 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error during code generation: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 147819 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Eyewear (Master)\n",
      "Starting analysis for sheet: Eyewear (Master)\n",
      "Fetching 50xN chunk from sheet: Eyewear (Master)\n",
      "Fetched chunk from sheet 'Eyewear (Master)':\n",
      "None, None, None, None, None, L30 Shopify Performance, None, None, None, Inventory On Hand, None, Running Inventory Forecast, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, No...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To achieve the tasks outlined, we will write a Python script that uses the `pandas` library to read the Excel file, identify the tables, and save them as separate CSV files. Below is the code that accomplishes this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Eyewear (Master)'\n",
      "\n",
      "# Define the output directory\n",
      "output_dir = '/Users/ajay/Documen...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Eyewear (Master)'\n",
      "\n",
      "# Define the output directory\n",
      "output_dir = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Eyewear (Master)'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "data = pd.read_excel(file_pat...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Eyewear (Master)'\n",
      "\n",
      "# Define the output directory\n",
      "output_dir = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Eyewear (Master)'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Initialize variables to store tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the rows of the DataFrame\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are None)\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it as a table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset current_table for the next table\n",
      "    else:\n",
      "        # Append the row to the current table\n",
      "        current_table.append(row.tolist())\n",
      "\n",
      "# Check if there's any remaining data in current_table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the output file name\n",
      "    output_file = os.path.join(output_dir, f'table_{i + 1}.csv')\n",
      "    # Save the table to CSV\n",
      "    table.to_csv(output_file, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_dir}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Eyewear (Master)'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Eyewear (Master)'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Total Inventory\n",
      "Starting analysis for sheet: Total Inventory\n",
      "Fetching 50xN chunk from sheet: Total Inventory\n",
      "Fetched chunk from sheet 'Total Inventory':\n",
      "EOM Inventory, $, Total Units, None, None, SKU, Planning SKU, Category, Collection, Product Name, Priority, COGS, Total, Total, Outerspace, Amazon, Rx, Inbound\n",
      "Date, Total Value, On Hand, Inbound, None, =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(F3:F1888)\"),2.0), =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(G3:G1888)\"),0.0), =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(H3:H1888)\"),1.0), =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(I3:I1888)\"),1.0), =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(J3:J1...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will create a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the 'Total Inventory' sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that fulfills your requirements:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet\n",
      "df = pd.read_excel(excel_file_pat...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified sheet\n",
      "df = pd.read_excel(excel_file_path, sheet_name='Total Inventory')\n",
      "\n",
      "# Identify tables by checking for rows with all NaN values\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_directory, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_directory}'.\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Sheet104\n",
      "Starting analysis for sheet: Sheet104\n",
      "Fetching 50xN chunk from sheet: Sheet104\n",
      "Fetched chunk from sheet 'Sheet104':\n",
      "\n",
      "\n",
      "Date, Total Value, On Hand, Inbound, None, 1472.0, 120.0, 9.0, 84.0, 1302.0, None, None, 0.0, 0.0, =SUM(O5:O1585), 51158.0, 0.0, 0.0, 11368.0, 214.0, 0.0, 0.0, 0.0, None, 0.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
      "None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the specified sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that fulfills the requirements:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysi...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the specified sheet from the Excel file\n",
      "df = pd.read_excel(excel_file_path...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the specified sheet from the Excel file\n",
      "df = pd.read_excel(excel_file_path, sheet_name='Sheet104', header=None)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    # Check if the row is empty (all elements are None)\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it as a table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        # Append the current row to the current table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_directory, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_directory}'.\")...\n",
      "Execution result:\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 2 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Outerspace\n",
      "Starting analysis for sheet: Outerspace\n",
      "Fetching 50xN chunk from sheet: Outerspace\n",
      "Fetched chunk from sheet 'Outerspace':\n",
      "Product Name, Color, Size, SKU, Location, Available, On Hand, Reserved, Deposco Reserved, Qty on Purchase Order, Shopify Link, Last Updated\n",
      "The Delta Single - Black/Matte Black, None, Large, 1Bla-BlaL-B45, NJ01, 1.0, 1.0, 0.0, 0.0, 0.0, 17157100000000.0, 2024-12-02 10:54:56 UTC\n",
      "The Delta Single - Black/Matte Black, None, Regular, 1Bla-BlaM-B36, NJ01, 0.0, 0.0, 0.0, 0.0, 0.0, 17157100000000.0, 2024-11-05 12:54:26 UTC\n",
      "The Delta Single - Black and Rose Gold, None, Large, 1Bla-RgL-B44, NJ01, 52.0, 5...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we can use the `pandas` library in Python, which is excellent for data manipulation and analysis, especially when dealing with Excel files and CSV outputs. Below is the Python code that reads the specified Excel file, extracts the data from the 'Outerspace' sheet, identifies tables, and saves them as separate CSV files in the specified directory.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Docum...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Outerspace'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Outerspace'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify ta...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Outerspace'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Outerspace'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# Assuming that the data is well-structured and there are no completely empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in data.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected a table, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the output file name\n",
      "    output_file = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    # Save the table to CSV\n",
      "    table.to_csv(output_file, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Outerspace'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Outerspace'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: OS Receivings Report\n",
      "Starting analysis for sheet: OS Receivings Report\n",
      "Fetching 50xN chunk from sheet: OS Receivings Report\n",
      "Fetched chunk from sheet 'OS Receivings Report':\n",
      "PO Number, PO Source, Created Date, Planned Arrival Date, Ship To, Total Ordered Quantity, Total Received Quantity, PO Status, PO Line Number, SKU, Product Name, Quantity Ordered, Quantity Received, Line Item Status, Pending Quantity\n",
      "Scout_UPS1ZC6J2000322660876, mc, 2024-10-10 00:00:00, 2024-10-14 00:00:00, NJ01, 144.0, 184.0, Received, 3.0, Bur-Tan-Sc03, The Scout Titanium - Bloodline (Watch Only), 50.0, 70.0, Received, =SWITCH(N2,\"Received\",0,\"New\",L2,\"Partial Receipt\",L2-M2)\n",
      "Scout_UPS1ZC6J200...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the data from the specified sheet. The script will identify contiguous blocks of data (tables) and save each table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code to achieve this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'OS Receivings Report'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/OS Receivings Report'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, sheet_name=sheet_na...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'OS Receivings Report'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/OS Receivings Report'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# Assuming that a table is defined as contiguous non-empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If current_table is not empty, save it\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    csv_file_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    table.to_csv(csv_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/OS Receivings Report'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/OS Receivings Report'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: WIP\n",
      "Starting analysis for sheet: WIP\n",
      "Fetching 50xN chunk from sheet: WIP\n",
      "Fetched chunk from sheet 'WIP':\n",
      "Category, =IFERROR(__xludf.DUMMYFUNCTION(\"IMPORTRANGE(\"\"https://docs.google.com/spreadsheets/d/1xFRANJyd5G4jlQfSJ5T8KZxBy6tYTtN39JVONrh-X8M/edit#gid=965814559\"\",\"\"POs!A:K\"\")\"),\"Collection\"), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),\"Product Name\"), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),\"SKU\"), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),\"PO Quantity\"), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_VALUE\"\"\"),\"Supplier\"), =IFERROR(__xludf.DUMMYFUNCTION(\"\"\"COMPUTED_...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the 'WIP' sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that performs these tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Da...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'WIP'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/WIP'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name, hea...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'WIP'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/WIP'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
      "\n",
      "# Initialize a list to hold the tables\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "# Iterate through the rows of the DataFrame\n",
      "for index, row in data.iterrows():\n",
      "    # Check if the row is empty (all elements are NaN)\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it to tables\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []  # Reset current_table for the next table\n",
      "    else:\n",
      "        # Append the row to the current table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the output file name\n",
      "    output_file = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    # Save the table to a CSV file\n",
      "    table.to_csv(output_file, index=False, header=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_folder}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/WIP'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/WIP'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Copy of WIP\n",
      "Starting analysis for sheet: Copy of WIP\n",
      "Fetching 50xN chunk from sheet: Copy of WIP\n",
      "Fetched chunk from sheet 'Copy of WIP':\n",
      "Category, Collection, Product Name, SKU, PO Quantity, Supplier, Year, PO, EXW, On hand OS, QC Start Date, PO Finish Date \n",
      "*MUST BE A MONDAY*, After 11/11 or not\n",
      "Eyewear, Villa, The Villa - Matte Black , VBla-Bla-G36, 350.0, JS, 2023.0, 02-13-JS-3A, 18.78, =iferror(vlookup(D2,Outerspace!D:F,3,)), None, 2023-05-23 00:00:00, =if(L2>=\"11/11/2024\"+0,\"yes\",\"no\")\n",
      "Watches, Rogue, The Rogue - Gunmetal/Red - Without Engraving, GraS-Bla-X12, 450.0, XS, 2023.0, 11-01-XS-4A, 50.190000000000005, =iferror(vloo...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the specified sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code to achieve this:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company ...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Copy of WIP'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the specified sheet from the Excel file\n",
      "df = pd.read_excel(excel_file_p...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the path to the Excel file and the output directory\n",
      "excel_file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "output_directory = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Copy of WIP'\n",
      "\n",
      "# Create the output directory if it doesn't exist\n",
      "os.makedirs(output_directory, exist_ok=True)\n",
      "\n",
      "# Read the specified sheet from the Excel file\n",
      "df = pd.read_excel(excel_file_path, sheet_name='Copy of WIP')\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# We will assume that a table is defined as a contiguous block of non-empty rows\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    if row.isnull().all():  # Check if the row is completely empty\n",
      "        if current_table:  # If we have collected rows for a table\n",
      "            tables.append(pd.DataFrame(current_table))  # Save the current table\n",
      "            current_table = []  # Reset for the next table\n",
      "    else:\n",
      "        current_table.append(row)  # Add the row to the current table\n",
      "\n",
      "# If there's any remaining data in current_table, add it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the output file name\n",
      "    output_file_path = os.path.join(output_directory, f'table_{i + 1}.csv')\n",
      "    # Save the table to a CSV file\n",
      "    table.to_csv(output_file_path, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them as CSV files in '{output_directory}'.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Copy of WIP'.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them as CSV files in '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Copy of WIP'.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Total Inventory_AH\n",
      "Starting analysis for sheet: Total Inventory_AH\n",
      "Fetching 50xN chunk from sheet: Total Inventory_AH\n",
      "Fetched chunk from sheet 'Total Inventory_AH':\n",
      "EOM Inventory, $, Total Units, None, None, SKU, Planning SKU, Category, Collection, Product Name, Priority, COGS, Total, Total, Outerspace, Amazon, Rx, Inbound, Shipmonk Available (FL), VinceroFLClosing2, NV Inventory Removal_2, NV Inventory Removal_3, Coming EOY (11-EOY), Transfer 1-3 Variance (Found), None, None, None, None, None, None, None, None, None, 0.1\n",
      "Date, Total Value, On Hand, Inbound, None, =IFERROR(__xludf.DUMMYFUNCTION(\"COUNTUNIQUE(F3:F1888)\"),1502.0), =IFERROR(__xludf.DUMMYFUNCTIO...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will create a Python script that uses the `pandas` library to read the specified Excel file and extract the tables from the specified sheet. The script will then save each identified table as a separate CSV file in the specified directory.\n",
      "\n",
      "Here's the Python code that performs the required operations:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Total Inventory_AH'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory_AH'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Total Inventory_AH'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory_AH'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the Excel file\n",
      "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "tables = []\n",
      "current_table = []\n",
      "\n",
      "for index, row in df.iterrows():\n",
      "    # Check if the row is empty (all elements are NaN)\n",
      "    if row.isnull().all():\n",
      "        # If current_table is not empty, save it as a table\n",
      "        if current_table:\n",
      "            tables.append(pd.DataFrame(current_table))\n",
      "            current_table = []\n",
      "    else:\n",
      "        # Append the current row to the current table\n",
      "        current_table.append(row)\n",
      "\n",
      "# If there's any remaining data in current_table, save it as the last table\n",
      "if current_table:\n",
      "    tables.append(pd.DataFrame(current_table))\n",
      "\n",
      "# Save each table as a separate CSV file\n",
      "for i, table in enumerate(tables):\n",
      "    # Define the output file name\n",
      "    output_file = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
      "    # Save the table to CSV\n",
      "    table.to_csv(output_file, index=False)\n",
      "\n",
      "print(f\"Extracted {len(tables)} tables and saved them to {output_folder}.\")...\n",
      "Execution result:\n",
      "Extracted 1 tables and saved them to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory_AH.\n",
      "\n",
      "Code executed successfully!\n",
      "Extracted 1 tables and saved them to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory_AH.\n",
      "\n",
      "---------------------------------------- \n",
      "Analyzing Sheet: Holiday Sales check data\n",
      "Starting analysis for sheet: Holiday Sales check data\n",
      "Fetching 50xN chunk from sheet: Holiday Sales check data\n",
      "Fetched chunk from sheet 'Holiday Sales check data':\n",
      "product_title, variant_sku, ordered_item_quantity, returned_item_quantity, net_quantity\n",
      "Eros Limited Release - Rose + Taupe, Rg-Tau-E44, 857.0, -37.0, 821.0\n",
      "Gift Bag, GIFT-Bag, 368.0, -18.0, 350.0\n",
      "Star Of David Pendant - Black Onyx, Jew-Sil-XN27, 314.0, -28.0, 286.0\n",
      "The Chrono S2 - Gunmetal/Walnut, Gra-Bro-S21, 301.0, -12.0, 289.0\n",
      "Eros Petite - Rose Gold/Taupe, Rg-Tau-3E44, 247.0, -20.0, 227.0\n",
      "The Altitude - Graphite, Gra-Bla-A22, 246.0, -10.0, 237.0\n",
      "Stone Cross Pendant - Black Onyx, Sil-Cros-XN...\n",
      "Analyzing the data and generating Python code...\n",
      "Sending data to LLM for analysis and code generation...\n",
      "Full LLM response:\n",
      "To accomplish the tasks you've outlined, we will write a Python script that uses the `pandas` library to read the specified Excel file and extract the data from the specified sheet. The script will then save the extracted tables as separate CSV files in the specified directory.\n",
      "\n",
      "Here's the Python code that performs the required tasks:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inven...\n",
      "Extracted code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Holiday Sales check data'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Holiday Sales check data'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_e...\n",
      "Executing the following code:\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Define the file path and sheet name\n",
      "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
      "sheet_name = 'Holiday Sales check data'\n",
      "output_folder = '/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Holiday Sales check data'\n",
      "\n",
      "# Create the output folder if it doesn't exist\n",
      "os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "# Read the data from the specified Excel sheet\n",
      "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
      "\n",
      "# Identify tables by checking for contiguous blocks of data\n",
      "# Assuming that the data is well-structured and there are no blank rows in the provided chunk\n",
      "# If there are blank rows, we would need to handle them accordingly\n",
      "\n",
      "# For this example, we will treat the entire data as a single table\n",
      "# If there were blank rows, we could use the following approach to split:\n",
      "# tables = [group for _, group in data.groupby((data.isnull().all(axis=1)).cumsum()) if not group.empty]\n",
      "\n",
      "# Save the extracted table as a CSV file\n",
      "output_file_path = os.path.join(output_folder, 'holiday_sales_check_data.csv')\n",
      "data.to_csv(output_file_path, index=False)\n",
      "\n",
      "print(f\"Data has been successfully extracted and saved to {output_file_path}\")...\n",
      "Execution result:\n",
      "Data has been successfully extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Holiday Sales check data/holiday_sales_check_data.csv\n",
      "\n",
      "Code executed successfully!\n",
      "Data has been successfully extracted and saved to /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Holiday Sales check data/holiday_sales_check_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load Excel Workbook\n",
    "file_path = '/Users/ajay/Documents/Atomic/inventory_analysis/Data/Company 3 - Inventory Dashboard _V2.xlsx'\n",
    "workbook_name = os.path.splitext(os.path.basename(file_path))[0]  # Get workbook name without extension\n",
    "workbook = load_workbook(file_path, data_only=False, read_only=True)\n",
    "sheet_names = [sheet_name for sheet_name in workbook.sheetnames if not workbook[sheet_name].sheet_state == 'hidden']\n",
    "\n",
    "# Create a base directory with the workbook name\n",
    "base_dir = os.path.join(os.getcwd(), workbook_name)\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract a 50xN chunk from a sheet\n",
    "def extract_sheet_chunk(sheet_name):\n",
    "    sheet_name = sheet_name.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    if sheet_name not in sheet_names:\n",
    "        log_message = f\"Sheet '{sheet_name}' doesn't exist. Available sheets: {sheet_names}\"\n",
    "        print(log_message)\n",
    "        return log_message\n",
    "    print(f\"Fetching 50xN chunk from sheet: {sheet_name}\")\n",
    "    sheet = workbook[sheet_name]\n",
    "    max_cols = sheet.max_column\n",
    "    data = [[cell.value for cell in row] for row in sheet.iter_rows(min_row=1, max_row=50, max_col=max_cols)]\n",
    "    formatted_chunk = \"\\n\".join([\", \".join(map(str, row)) for row in data])\n",
    "    print(f\"Fetched chunk from sheet '{sheet_name}':\\n{formatted_chunk[:500]}...\")  # Show first 500 chars for brevity\n",
    "    return formatted_chunk\n",
    "\n",
    "# Function to analyze a chunk and generate code\n",
    "import re\n",
    "\n",
    "def analyze_and_generate_code(sheet_name: str, chunk: str, sheet_folder: str) -> str:\n",
    "    llm_input = f\"\"\"\n",
    "    You are a Python code generator for table extraction.\n",
    "    Below is a 50xN chunk of data from the sheet '{sheet_name}' in an Excel file.\n",
    "\n",
    "    {chunk}\n",
    "\n",
    "    Tasks:\n",
    "    1. Analyze the data to identify all tables present in the chunk. Tables are defined as contiguous blocks of data separated by blank rows or rows with no data.\n",
    "    2. Generate Python code that:\n",
    "       - Reads data from the provided Excel file ('{file_path}') and specified sheet ('{sheet_name}').\n",
    "       - Saves all extracted tables as separate CSV files in the folder '{sheet_folder}'.\n",
    "       - Ensures the generated code creates the CSV files in the exact folder specified and avoids creating additional folders elsewhere.\n",
    "    3. Do not make any assumptions about the meaning or functionality of specific columns unless explicitly mentioned.\n",
    "    \"\"\"\n",
    "    print(f\"Sending data to LLM for analysis and code generation...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    try:\n",
    "        response = llm.invoke(llm_input)\n",
    "        generated_response = response.content  # Extract string content\n",
    "        print(f\"Full LLM response:\\n{generated_response[:500]}...\")  # Log full response\n",
    "\n",
    "        # Extract Python code block using regex\n",
    "        match = re.search(r\"```python(.*?)```\", generated_response, re.DOTALL)\n",
    "        if match:\n",
    "            generated_code = match.group(1).strip()  # Extract the Python code\n",
    "            print(f\"Extracted code:\\n{generated_code[:500]}...\")  # Log extracted code\n",
    "            return generated_code\n",
    "        else:\n",
    "            print(\"No valid Python code block found in the LLM response.\")\n",
    "            return \"Error: No valid Python code block found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code generation: {e}\")\n",
    "        return f\"Error during code generation: {e}\"\n",
    "\n",
    "# Function to execute generated Python code\n",
    "def execute_code(code: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executing the following code:\\n{code}...\")  # Log the extracted code\n",
    "        repl_tool = PythonREPLTool()\n",
    "        result = repl_tool.run(code)\n",
    "        print(\"Execution result:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"Execution Error: {str(e)}\"\n",
    "        print(error_message)\n",
    "        print(traceback.format_exc())\n",
    "        return error_message\n",
    "\n",
    "# Main execution process\n",
    "def run_analysis(sheet_name):\n",
    "    print(f\"Starting analysis for sheet: {sheet_name}\")\n",
    "    \n",
    "    # Step 1: Fetch sheet chunk\n",
    "    chunk = extract_sheet_chunk(sheet_name)\n",
    "    if \"doesn't exist\" in chunk:\n",
    "        print(f\"Error: {chunk}\")\n",
    "        return chunk\n",
    "\n",
    "    # Step 2: Create the sheet folder\n",
    "    sheet_folder = os.path.join(base_dir, sheet_name)\n",
    "    os.makedirs(sheet_folder, exist_ok=True)\n",
    "\n",
    "    # Step 3: Analyze and generate code\n",
    "    print(\"Analyzing the data and generating Python code...\")\n",
    "    code = analyze_and_generate_code(sheet_name, chunk, sheet_folder)\n",
    "    if \"Error during code generation\" in code:\n",
    "        return code\n",
    "\n",
    "    # Step 4: Attempt to execute code with retries\n",
    "    retry = 10\n",
    "    while retry > 0:\n",
    "        result = execute_code(code)\n",
    "        if \"Execution Error\" not in result:\n",
    "            print(\"Code executed successfully!\")\n",
    "            return result  # Successful execution\n",
    "\n",
    "        # Provide feedback and request fixes\n",
    "        error_message = f\"\\n\\nError Message: {result}\"\n",
    "        print(f\"Retrying... ({10 - retry + 1}/10)\")\n",
    "        print(f\"Requesting LLM to fix the code. Error: {error_message}\")\n",
    "        code = analyze_and_generate_code(sheet_name, chunk, sheet_folder)\n",
    "        retry -= 1\n",
    "\n",
    "    print(\"Exceeded maximum retries. Could not execute code successfully.\")\n",
    "    return \"Failed after multiple retries.\"\n",
    "\n",
    "# Run the workflow for each sheet\n",
    "for sheet_to_analyze in sheet_names:\n",
    "    print('-'*40, f'\\nAnalyzing Sheet: {sheet_to_analyze}')\n",
    "    print(run_analysis(sheet_to_analyze))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost: 0.02 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/RLS/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_2.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ing and management.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_4.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_5.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_7.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Dry Ice Forecast/table_6.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Inbound Receipts/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU List/SKU_List.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen tool/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen tool/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/DC Inventory/table_1.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 9 column 28 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...t inventory levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet98/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Orders shipped/orders_shipped.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet99/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Sheet99/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Packaging depletions/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Replen Orders/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Share by SKU/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/DC Fcst/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/DC Fcst/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Credit Requests/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Total Inventory/table_2.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ing and management.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ing and management.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Total Inventory/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/SKU depletions/table_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/n6fpg8f54xb5ld5v6bg1wgl80000gn/T/ipykernel_36661/1522898781.py:94: DtypeWarning: Columns (1,2,3,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Rollling Sales Forecast/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Rollling Sales Forecast/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/NW/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Hf market SKU breakdown/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/Hf market SKU breakdown/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/CP Forecast (2023)/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/CP Forecast (2023)/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management/CP Forecast (2023)/table_1.csv\n",
      "Results saved to Company 2 - Supply Management.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_2.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ng inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ng inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_3.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_4.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_5.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_7.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla... forecasting needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla... stock effectively.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_6.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 7\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 6\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla... sales forecasting.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 5\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 4\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 3\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...anage stock levels.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 2\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla... sales forecasting.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 1\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla... sales forecasting.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 0\n",
      "Max retries reached. Skipping this sheet.\n",
      "Failed to analyze file: table_6.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_12.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_13.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_8.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 7\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 6\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 5\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 4\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_11.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_10.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...future stock needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_9.csv\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...g future purchases.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 9\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ventory management.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 8\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 7\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 6\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...future stock needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 5\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 4\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 3\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...sting future needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 2\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...ventory management.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 1\n",
      "Validation Error: 1 validation error for InventorySheetAnalysis\n",
      "  Invalid JSON: expected ident at line 5 column 32 [type=json_invalid, input_value='{\\n    \"is_inventory_pla...re inventory needs.\"\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
      "Retries left: 0\n",
      "Max retries reached. Skipping this sheet.\n",
      "Failed to analyze file: table_9.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Forecast/table_14.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Copy of WIP/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory_AH/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Carry (Master)/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Outerspace/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/OS Receivings Report/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Product/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Total Inventory/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Eyewear (Master)/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/WIP/table_1.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Holiday Sales check data/holiday_sales_check_data.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104/table_2.csv\n",
      "Processing file: /Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2/Sheet104/table_1.csv\n",
      "Results saved to Company 3 - Inventory Dashboard _V2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the structured output schema\n",
    "class InventorySheetAnalysis(BaseModel):\n",
    "    is_inventory_planning: int = Field(\n",
    "        ..., description=\"1 if the sheet is for inventory planning, 0 otherwise\"\n",
    "    )\n",
    "    details: dict = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A dictionary mapping columns to roles: SKU, Location/Warehouse, Quantity, \"\n",
    "            \"Total Inventory, Current Inventory, Sales Forecast. Set to None if not found.\"\n",
    "        ),\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., description=\"Explanation of why this sheet is for inventory planning.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to clean the LLM response\n",
    "def clean_response(response: str) -> str:\n",
    "    if response.startswith(\"```json\"):\n",
    "        response = response[7:]  # Remove ```json\n",
    "    if response.endswith(\"```\"):\n",
    "        response = response[:-3]  # Remove ```\n",
    "    return response.strip()  # Strip any extra whitespace\n",
    "\n",
    "\n",
    "# Function to analyze a sheet using GPT-4o-mini\n",
    "def analyze_sheet(dataframe, llm, retries=10):\n",
    "    markdown_preview = dataframe.head(5).to_markdown()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data analysis expert. Below is a preview of a sheet:\n",
    "\n",
    "    {markdown_preview}\n",
    "\n",
    "    Task:\n",
    "    1. Determine if this sheet is used for inventory planning (1 for Yes, 0 for No).\n",
    "    2. If Yes, provide:\n",
    "       - 'details': A dictionary mapping column names to these roles: SKU, Location/Warehouse, Quantity, Total Inventory, Current Inventory, Sales Forecast. If no related column exists, set the value to None.\n",
    "       - 'description': A brief explanation explaining why this sheet is for inventory planning.\n",
    "\n",
    "    Respond strictly in the following JSON format:\n",
    "    {{\n",
    "        \"is_inventory_planning\": 1,\n",
    "        \"details\": {{\n",
    "            \"SKU\": \"Column Name or None\",\n",
    "            \"Location/Warehouse\": \"Column Name or None\",\n",
    "            \"Quantity\": \"Column Name or None\",\n",
    "            \"Total Inventory\": \"Column Name or None\",\n",
    "            \"Current Inventory\": \"Column Name or None\",\n",
    "            \"Sales Forecast\": \"Column Name or None\"\n",
    "        }},\n",
    "        \"description\": \"Brief explanation\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = llm.predict(prompt)\n",
    "            cleaned_response = clean_response(response)\n",
    "            structured_output = InventorySheetAnalysis.model_validate_json(cleaned_response)\n",
    "            return structured_output\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Validation Error: {ve}\")\n",
    "            retries -= 1\n",
    "            print(f\"Retries left: {retries}\")\n",
    "            if retries == 0:\n",
    "                print(\"Max retries reached. Skipping this sheet.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Function to process all CSV files in a base folder\n",
    "def analyze_csv_files(base_folder, llm):\n",
    "    results = []\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        folder_path = os.path.join(base_folder, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    response = analyze_sheet(dataframe=df, llm=llm)\n",
    "                    if response:\n",
    "                        # Unpack the details dictionary into individual columns\n",
    "                        details = response.details\n",
    "                        results.append(\n",
    "                            {\n",
    "                                'sheet_name': folder,\n",
    "                                \"file\": file,\n",
    "                                \"is_inventory_planning\": response.is_inventory_planning,\n",
    "                                \"description\": response.description,\n",
    "                                \"SKU\": details.get(\"SKU\"),\n",
    "                                \"Location/Warehouse\": details.get(\"Location/Warehouse\"),\n",
    "                                \"Quantity\": details.get(\"Quantity\"),\n",
    "                                \"Total Inventory\": details.get(\"Total Inventory\"),\n",
    "                                \"Current Inventory\": details.get(\"Current Inventory\"),\n",
    "                                \"Sales Forecast\": details.get(\"Sales Forecast\"),\n",
    "                            }\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"Failed to analyze file: {file}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "base_folders = [\"/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 2 - Supply Management\", \"/Users/ajay/Documents/Atomic/inventory_analysis_2/Company 3 - Inventory Dashboard _V2\"]\n",
    "for base_folder in base_folders:\n",
    "    analysis_results_df = analyze_csv_files(base_folder, llm)\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    output_file = f\"{base_folder.split('/')[-1]}.csv\"\n",
    "    analysis_results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    # print(analysis_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crustdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
